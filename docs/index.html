<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>MACHINE LEARNING BASED RECONSTRUCTION IN A PIXELLATED LIQUID ARGON TIME
PROJECTION CHAMBER</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="main.tex"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
><div class="maketitle">
                                                                                         
                                                                                         
                                                                                         
                                                                                         

<h2 class="titleHead">MACHINE LEARNING BASED RECONSTRUCTION IN A PIXELLATED LIQUID ARGON
TIME PROJECTION CHAMBER</h2>
<div class="author" ></div><br />
<div class="date" ></div>
      </div>
<!--l. 219--><p class="indent" >      &#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />
<div class="center" 
>
<!--l. 220--><p class="noindent" >
<!--l. 221--><p class="noindent" >by</div>
<div class="center" 
>
<!--l. 224--><p class="noindent" >
<!--l. 225--><p class="noindent" >Orgho Neogi</div>
<!--l. 227--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />
<div class="center" 
>
<!--l. 228--><p class="noindent" >
<!--l. 229--><p class="noindent" >A thesis submitted in partial fulfillment<br />
of the requirements for the Doctor of Philosophy<br />
degree in Physics &amp; Astronomy&#x00A0;in the<br />
Graduate College of<br />
The University of Iowa</div>
<div class="center" 
>
<!--l. 236--><p class="noindent" >
                                                                                         
                                                                                         
<!--l. 237--><p class="noindent" >May&#x00A0;2024</div>
<div class="center" 
>
<!--l. 240--><p class="noindent" >
<!--l. 241--><p class="noindent" >Thesis Committee: Name of Thesis Supervisor, Jane Nachtman</div>
<div class="center" 
>
<!--l. 243--><p class="noindent" >
<!--l. 244--><p class="noindent" >Yaser Onel</div>
<div class="center" 
>
<!--l. 246--><p class="noindent" >
<!--l. 247--><p class="noindent" >Milind Diwan</div>
<div class="center" 
>
<!--l. 249--><p class="noindent" >
<!--l. 250--><p class="noindent" >Mary Hall Reno</div>
                                                                                         
                                                                                         
<blockquote class="quote">
<!--l. 4--><p class="noindent" >I have yet to see any problem, however complicated, which, when looked at in the right way did
not become still more complicated.
<!--l. 7--><p class="noindent" >Poul Anderson<br />
</blockquote>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<div class="center" 
>
<!--l. 2--><p class="noindent" >
<!--l. 3--><p class="noindent" >ABSTRACT </div>
<!--l. 7--><p class="indent" >      The Deep Underground Neutrino Experiment (DUNE) will address open issues in neutrino physics
such as the measurement of the CP-violating phase in neutrino oscillations and the neutrino mass
ordering. The 2x2 demonstrator is a single-phase liquid argon time projection chamber (LArTPC), with
four modules, operated as a prototype for the DUNE Liquid Argon Near Detector (ND-LAr). Based on the
ArgonCube design concept, the 2x2 features a novel pixelated charge readout and advanced high-coverage
photon detection system.
<!--l. 11--><p class="indent" >      Machine learning (specifically the SPINE package) can be used to form a complete reconstruction
pipeline of the 2x2 events. This paper will describe the workings on this reconstruction and its current
performance.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-1000"></a>table of contents</h3>
      <div class="tableofcontents">
      &#x00A0;<span class="sectionToc" ><a 
href="#Q1-1-3">List of Tables</a></span>
<br />      &#x00A0;<span class="sectionToc" ><a 
href="#Q1-1-5">List of Figures</a></span>
<br />      &#x00A0;<span class="sectionToc" >1 <a 
href="#x1-40001" id="QQ2-1-6">Introduction</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.1 <a 
href="#x1-50001.1" id="QQ2-1-7">The Standard Model</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.2 <a 
href="#x1-60001.2" id="QQ2-1-11">Electroweak Theory</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.3 <a 
href="#x1-70001.3" id="QQ2-1-12">neutrino</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.4 <a 
href="#x1-80001.4" id="QQ2-1-13">Neutrino Mass Ordering</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.5 <a 
href="#x1-90001.5" id="QQ2-1-14">Neutrino Oscillations</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.6 <a 
href="#x1-100001.6" id="QQ2-1-15">Dirac Vs MAJORANA Neutrinos</a></span>
<br />      &#x00A0;<span class="sectionToc" >2 <a 
href="#x1-110002" id="QQ2-1-16">Neutrino Detection</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >2.1 <a 
href="#x1-120002.1" id="QQ2-1-17">Types of Detectors</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >2.2 <a 
href="#x1-130002.2" id="QQ2-1-18">Deep Underground Neutrino Experiment (DUNE)</a></span>
<br />      &#x00A0;<span class="sectionToc" >3 <a 
href="#x1-140003" id="QQ2-1-23">Machine Learning</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.1 <a 
href="#x1-150003.1" id="QQ2-1-26">Perceptron Neuron</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.2 <a 
href="#x1-160003.2" id="QQ2-1-29">Sigmoid Neuron</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.3 <a 
href="#x1-170003.3" id="QQ2-1-31">Activation Functions</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.4 <a 
href="#x1-180003.4" id="QQ2-1-36">Neural Network</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.5 <a 
href="#x1-190003.5" id="QQ2-1-38">Gradient Descent</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.6 <a 
href="#x1-200003.6" id="QQ2-1-39">Convolutional Neural Networks</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.7 <a 
href="#x1-210003.7" id="QQ2-1-41">Graph Neural Networks</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.8 <a 
href="#x1-220003.8" id="QQ2-1-42">Model Development</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.9 <a 
href="#x1-230003.9" id="QQ2-1-44">Model Optimization</a></span>
<br />      &#x00A0;<span class="sectionToc" >4 <a 
href="#x1-240004" id="QQ2-1-45">Reconstruction using SPINE</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >4.1 <a 
href="#x1-250004.1" id="QQ2-1-48">Semantic Segmentation and PPN</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >4.2 <a 
href="#x1-260004.2" id="QQ2-1-52">Particle Clustering</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >4.3 <a 
href="#x1-270004.3" id="QQ2-1-55">Particle Identification</a></span>
<br />      &#x00A0;<span class="sectionToc" >5 <a 
href="#x1-280005" id="QQ2-1-58">Future Steps</a></span>
<br />      &#x00A0;<span class="sectionToc" ><a 
href="#Q1-1-60">References</a></span>
      </div>
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-2000"></a>List of Tables</h3>
<a 
 id="Q1-1-3"></a>
      <div class="tableofcontents"><span class="lotToc" >1&#x00A0;<a 
href="#x1-5005r1">Quark Flavors and Their Approximate Masses</a></span><br />
      </div>
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-3000"></a>List of Figures</h3>
<a 
 id="Q1-1-5"></a>
      <div class="tableofcontents"><span class="lofToc" >1&#x00A0;<a 
href="#x1-5003r1">The Elementary Particles in the Standard Model</a></span><br /><span class="lofToc" >2&#x00A0;<a 
href="#x1-5004r2">Quarks inside a proton.
Labelled u for up and d for down</a></span><br /><span class="lofToc" >3&#x00A0;<a 
href="#x1-13001r3">Cartoon of the DUNE setup</a></span><br /><span class="lofToc" >4&#x00A0;<a 
href="#x1-13002r4">Design of the
DUNE ND with <span 
class="zptmcm7t-x-x-120">5</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">7 </span>modules</a></span><br /><span class="lofToc" >5&#x00A0;<a 
href="#x1-13003r5">Cutaway image of a module</a></span><br /><span class="lofToc" >6&#x00A0;<a 
href="#x1-13004r6">Pictures of the
first module to be build (Module-0)</a></span><br /><span class="lofToc" >7&#x00A0;<a 
href="#x1-14001r7">Example of a basic decision tree</a></span><br /><span class="lofToc" >8&#x00A0;<a 
href="#x1-14002r8">Example
of a basic Neural Net</a></span><br /><span class="lofToc" >9&#x00A0;<a 
href="#x1-15003r9">Perceptron Neuron</a></span><br /><span class="lofToc" >10&#x00A0;<a 
href="#x1-15005r10">Perceptron network</a></span><br /><span class="lofToc" >11&#x00A0;<a 
href="#x1-16004r11">Sigmoid
Function</a></span><br /><span class="lofToc" >12&#x00A0;<a 
href="#x1-17002r12">tanh Function</a></span><br /><span class="lofToc" >13&#x00A0;<a 
href="#x1-17005r13">RELU function</a></span><br /><span class="lofToc" >14&#x00A0;<a 
href="#x1-17008r14">Leaky RELU function</a></span><br /><span class="lofToc" >15&#x00A0;<a 
href="#x1-17011r15">ELU
function</a></span><br /><span class="lofToc" >16&#x00A0;<a 
href="#x1-18001r16">Parts of a Neural Net</a></span><br /><span class="lofToc" >17&#x00A0;<a 
href="#x1-20001r17">Cartoon depicting layers of a CNN</a></span><br /><span class="lofToc" >18&#x00A0;<a 
href="#x1-22001r18">Model
development</a></span><br /><span class="lofToc" >19&#x00A0;<a 
href="#x1-24003r19">Some reconstructed events from the module-0 prototype for
the DUNE Near Detector</a></span><br /><span class="lofToc" >20&#x00A0;<a 
href="#x1-24006r20">Overview of the SPINE schematic</a></span><br /><span class="lofToc" >21&#x00A0;<a 
href="#x1-25001r21">UResNet
Architecture</a></span><br /><span class="lofToc" >22&#x00A0;<a 
href="#x1-25002r22">Semantic Segmentation Event display comparing monte carlo
truth and reconstructed prediction</a></span><br /><span class="lofToc" >23&#x00A0;<a 
href="#x1-25003r23">Confusion Matrix showing performance
of semantic segmentation</a></span><br /><span class="lofToc" >24&#x00A0;<a 
href="#x1-26001r24">Architecture of the clustering GNN</a></span><br /><span class="lofToc" >25&#x00A0;<a 
href="#x1-26002r25">Box and
whisker plot showing performance of clustering</a></span><br /><span class="lofToc" >26&#x00A0;<a 
href="#x1-27001r26">Particle ID Event display
comparing monte carlo truth and reconstructed prediction</a></span><br /><span class="lofToc" >27&#x00A0;<a 
href="#x1-27002r27">Confusion
Matrix showing performance of Particle Identification</a></span><br />
      </div>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-40001"></a>Introduction</h3>
<!--l. 3--><p class="noindent" >I have yet to see any problem, however complicated, which, when looked at in the right way did not
become still more complicated.
                                                                                         <div class="flushright" 
>
<!--l. 5--><p class="noindent" >
&#8212; Poul Anderson</div>
<!--l. 7--><p class="indent" >      People working in the field of high energy physics have a tendency to concern themselves with
attempting to solve problems that are incredibly complicated. So, perhaps, there is a touch
of irony that the problem that they are trying to solve is not only incredibly fundamental,
but also very simple to state. The question can be boiled down to &#8211; what is the stuff in our
universe made of? What immediately follows from this fundamental inquiry is - how is matter
made up of these things? Or to put it another way, how do the fundamental building blocks
interact?
<!--l. 12--><p class="indent" >      In some sense, particle physics tries to distill matter and the interactions therein down to the
smallest possible level to which it can be broken down. Turns out that breaking these concepts down to
this elementary level of specificity is an incredibly complicated process of which we have merely begun to
scratch the surface. As such, this paper focuses on a tiny fraction of these fundamental building blocks &#8211;
the elusive neutrino with the hope of just perhaps being able to untangle some of the myriad of secrets that
it harbours.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">1.1    </span> <a 
 id="x1-50001.1"></a>The Standard Model</h4>
                                                                                         
                                                                                         
<!--l. 3--><p class="noindent" >Before the protagonist <span class="footnote-mark"><a 
href="main2.html#fn1x0"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-5001f1"></a> 
of our story - the neutrino - can be formally introduced, the stage has to be set. A good candidate to set the
stage would be the standard model which describes three of the four known fundamental forces,
electromagnetic, weak and strong interactions (it struggles to deal with gravity) and classifying all known
elementary particles [<a 
href="#XOerter">1</a>]. Just like any foundational theory that undergirds a sub-field of a subject, the
standard model definitely wasn&#8217;t developed in a day and as such, there is definite value in becoming
familliar with the historical context surrounding the standard model in our quest to understand
neutrinos.
<!--l. 9--><p class="indent" >      One may definitely quibble about where our understanding of the fundamental particles starts from,
after all, humans have been trying to find out the nature of our universe and the things that make
it up going back as far as the 4th century BCE with Plato positing that everything is made
up of 4 elements (water, wind, earth and fire)[<a 
href="#XTimaeus">2</a>], but I think it makes sense to look at the
elementary particles that make up the standard model as we know it today &#8211; with the definite
understanding that there may very well be physics that lies beyond the realm of the standard
model.
<!--l. 11--><p class="indent" >      At its core, the Standard Model consists of two main categories of particles: fermions, which make
up matter, and bosons, which mediate interactions. Fermions have <span 
class="zptmcm7t-x-x-120">1</span><span 
class="zptmcm7m-x-x-120">&#x2215;</span><span 
class="zptmcm7t-x-x-120">2 </span>integer spins while bosons have
integer spins.
<!--l. 14--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 17--><p class="noindent" ><img 
src="figures/sm.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-5003r1"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;1. </span><span  
class="content">The Elementary Particles in the Standard Model                                    </span></div><!--tex4ht:label?: x1-5003r1 -->
                                                                                         
                                                                                         
<!--l. 20--><p class="indent" >      </div><hr class="endfigure">
<!--l. 22--><p class="indent" >      The fermions can be further categorized into quarks and leptons.
<!--l. 24--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 27--><p class="noindent" ><img 
src="figures/protonQuarks.png" alt="PIC"  
width="284" height="284" > <a 
 id="x1-5004r2"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;2. </span><span  
class="content">Quarks inside a proton. Labelled u for up and d for down                            </span></div><!--tex4ht:label?: x1-5004r2 -->
                                                                                         
                                                                                         
<!--l. 31--><p class="indent" >      </div><hr class="endfigure">
<!--l. 33--><p class="indent" >      Quarks are understood to be fundamental constituents of matter, forming the building blocks of
protons, neutrons, and other hadrons (Composite subatomic particles that are made up of at least 2
quarks). Quarks interact with each other via the strong force.We have so far discovered 6 flavors of quarks
&#8211; up (<img 
src="main0x.png" alt="u " class="math">), down (<img 
src="main1x.png" alt="d " class="math">), charm (<img 
src="main2x.png" alt="c " class="math">), strange (<img 
src="main3x.png" alt="s " class="math">), top (<img 
src="main4x.png" alt="t " class="math">), and bottom (<img 
src="main5x.png" alt="b " class="math">). Each flavor has different
mass. These masses and their interactions with other particles are crucial for the stability and properties of
atomic nuclei.
      <div class="table">
                                                                                         
                                                                                         
<!--l. 38--><p class="indent" >      <hr class="float"><div class="float" 
>
                                                                                         
                                                                                         
<div class="tabular">
 <table id="TBL-2" class="tabular" 
 
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"><col 
id="TBL-2-2"><col 
id="TBL-2-3"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-1"  
class="td11">Quark Flavor</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-1-2"  
class="td11">Approximate Mass (MeV/c<img 
src="main6x.png" alt="2  " class="math">)</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-1-3"  
class="td11">Charge (e)</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-1"  
class="td11">Up (u)            </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-2-2"  
class="td11">                  2.2 - 3.0</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-2-3"  
class="td11">     +<img 
src="main7x.png" alt="2
3  " class="math"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-1"  
class="td11">Down (d)       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-3-2"  
class="td11">                  4.7 - 5.0</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-3-3"  
class="td11">     -<img 
src="main8x.png" alt="13  " class="math"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-1"  
class="td11">Strange (s)     </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-4-2"  
class="td11">                  95 - 105</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-4-3"  
class="td11">     -<img 
src="main9x.png" alt="1
3  " class="math"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-1"  
class="td11">Charm (c)      </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-5-2"  
class="td11">               1270 - 1720</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-5-3"  
class="td11">     +<img 
src="main10x.png" alt="2
3  " class="math"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-1"  
class="td11">Bottom (b)     </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-6-2"  
class="td11">               4180 - 4380</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-6-3"  
class="td11">     -<img 
src="main11x.png" alt="1
3  " class="math"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-1"  
class="td11">Top (t)           </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-7-2"  
class="td11">           172000 - 173000</td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-7-3"  
class="td11">     +<img 
src="main12x.png" alt="23  " class="math"></td>

</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-1"  
class="td11">           </td></tr></table>                                                                          </div>
<a 
 id="x1-5005r1"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Table&#x00A0;1.
</span><span  
class="content">Quark
Flavors
and
Their
Approximate
Masses
</span></div><!--tex4ht:label?: x1-5005r1 -->
                                                                                         
                                                                                         
      </div><hr class="endfloat" />
      </div>
<!--l. 57--><p class="indent" >      Each type of fermion carries a specific flavor and generation. For instance, the electron (<span 
class="zptmcm7m-x-x-120">e</span>) belongs
to the first generation, while the muon (<span 
class="zptmcm7m-x-x-120">&#x03BC;</span>) and tau (<span 
class="zptmcm7m-x-x-120">&#x03C4;</span>) belong to the second and third generations, respectively
<span class="footnote-mark"><a 
href="main3.html#fn2x0"><sup class="textsuperscript">2</sup></a></span><a 
 id="x1-5006f2"></a>.
<!--l. 60--><p class="indent" >      The interactions between these particles are mediated by gauge bosons, which are the force
carriers. The Standard Model includes the following gauge bosons:
<!--l. 63--><p class="indent" >
<table 
class="align">
             <tr><td 
class="align-odd">Gauge Bosons:</td>             <td 
class="align-even">    Photon <span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">A</span><span 
class="zptmcm7t-x-x-120">)</span>   (mediates electromagnetic force)</td>                              <td 
class="align-label"><a 
 id="x1-5008r1"></a>(1)
             </td></tr><tr><td 
class="align-odd"></td>                           <td 
class="align-even">    W and Z bosons <span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">W</span><sup><span 
class="zptmcm7y-x-x-90">±</span></sup><span 
class="zptmcm7m-x-x-120">,Z</span><sup><span 
class="zptmcm7t-x-x-90">0</span></sup><span 
class="zptmcm7t-x-x-120">)</span>   (mediates weak force)</td>                          <td 
class="align-label"><a 
 id="x1-5009r2"></a>(2)
             </td></tr><tr><td 
class="align-odd"></td>                           <td 
class="align-even">    Gluons <span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">g</span><span 
class="zptmcm7t-x-x-120">)</span>   (mediates strong force)</td>                                       <td 
class="align-label"><a 
 id="x1-5010r3"></a>(3)             </td></tr></table>
<!--l. 69--><p class="indent" >      The mathematical framework underpinning the Standard Model is primarily based on gauge theory,
specifically the group <span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(3)</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(2)</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7m-x-x-120">U</span><span 
class="zptmcm7t-x-x-120">(1)</span>. Each of these groups corresponds to a different
force:
                                                                                         
                                                                                         
<!--l. 72--><p class="indent" >
<table 
class="align">
                    <tr><td 
class="align-odd">Strong Interaction:</td>                             <td 
class="align-even">    <span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(3)</span>   (color charge)</td>                                        <td 
class="align-label"><a 
 id="x1-5011r4"></a>(4)
                    </td></tr><tr><td 
class="align-odd">Weak Interaction:</td>                              <td 
class="align-even">    <span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(2)</span>   (isospin)</td>                                             <td 
class="align-label"><a 
 id="x1-5012r5"></a>(5)
                    </td></tr><tr><td 
class="align-odd">Electromagnetic Interaction:</td>                    <td 
class="align-even">    <span 
class="zptmcm7m-x-x-120">U</span><span 
class="zptmcm7t-x-x-120">(1)</span>   (hypercharge)</td>                                         <td 
class="align-label"><a 
 id="x1-5013r6"></a>(6)                    </td></tr></table>
<!--l. 78--><p class="indent" >      The Higgs mechanism, a crucial part of the Standard Model, provides a mass to the
W and Z bosons via spontaneous symmetry breaking. The Higgs field <span 
class="zptmcm7m-x-x-120">&#x03D5; </span>can be represented
as:
<!--l. 81--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                  <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">&#x03D5;</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">x</span><span 
class="zptmcm7t-x-x-120">)</span></td>                                  <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main13x.png" alt=" 1
&#x221A;---
  2"  class="frac" align="middle"><img 
src="main14x.png" alt="(         )
|    0    |
(         )
  v + h(x)"  class="left" align="middle"></td>                                                                    <td 
class="align-label"><a 
 id="x1-5014r7"></a>(7)                                  </td></tr></table>
<!--l. 85--><p class="indent" >      where <span 
class="zptmcm7m-x-x-120">v </span>is the vacuum expectation value and <span 
class="zptmcm7m-x-x-120">h</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">x</span><span 
class="zptmcm7t-x-x-120">) </span>is the physical Higgs boson field. The
mass terms for the gauge bosons arise when the Higgs field acquires a vacuum expectation
value.
      <h4 class="subsectionHead"><span class="titlemark">1.2    </span> <a 
 id="x1-60001.2"></a>Electroweak Theory</h4>
<!--l. 3--><p class="noindent" >Moving on to the other fundamental forces, the combination of electromagnetism and the weak force into
one consistent theory was monumental in pushing physics forward.The journey towards electroweak
unification began with the discovery of the weak force, a crucial interaction responsible for processes like
beta decay. Early experiments revealed that the weak force was much weaker than electromagnetism and
had a very short range. It was eventually understood that the weak force and electromagnetism were
manifestations of a more fundamental interaction.
<!--l. 7--><p class="indent" >      In the 1970s, Sheldon Glashow, Abdus Salam, and Steven Weinberg formulated the electroweak
theory, which successfully unified these two interactions into a single theoretical framework. Their theory
predicted the existence of the W and Z bosons, which mediate the weak force. The electroweak theory is
based on the gauge symmetry group <span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(2)</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7m-x-x-120">U</span><span 
class="zptmcm7t-x-x-120">(1)</span><sub><span 
class="zptmcm7m-x-x-90">Y</span> </sub>.
<!--l. 11--><p class="indent" >      The Lagrangian for the electroweak interaction can be written as
<!--l. 13--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                            <tr><td 
class="align-odd"><span 
class="zptmcm7y-x-x-120"><img 
src="zptmcm7y-c-4c.png" alt="L" class="-120x-x-4c" /></span><sub>EW</sub></td>                            <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7y-x-x-120">-</span><img 
src="main15x.png" alt="1-
4"  class="frac" align="middle"><span 
class="zptmcm7m-x-x-120">W</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;&#x03BD;</span></sub><sup><span 
class="zptmcm7m-x-x-90">i</span></sup><span 
class="zptmcm7m-x-x-120">W</span><sub>
<span 
class="zptmcm7m-x-x-90">i</span></sub><sup><span 
class="zptmcm7m-x-x-90">&#x03BC;&#x03BD;</span></sup><span 
class="zptmcm7y-x-x-120">-</span><img 
src="main16x.png" alt="1-
4"  class="frac" align="middle"><span 
class="zptmcm7m-x-x-120">B</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;&#x03BD;</span></sub><span 
class="zptmcm7m-x-x-120">B</span><sup><span 
class="zptmcm7m-x-x-90">&#x03BC;&#x03BD;</span></sup></td>                                                           <td 
class="align-label"><a 
 id="x1-6001r8"></a>(8)
                            </td></tr><tr><td 
class="align-odd"></td>                                  <td 
class="align-even">     <span 
class="zptmcm7t-x-x-120">+</span><img 
src="main17x.png" alt="1
--
2"  class="frac" align="middle"><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">W</span> </sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup><span 
class="zptmcm7m-x-x-120">W</span><sub>
<span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub><sup><span 
class="zptmcm7m-x-x-90">i</span></sup><span 
class="zptmcm7m-x-x-120">W</span><sup><span 
class="zptmcm7m-x-x-90">i&#x03BC;</span></sup><span 
class="zptmcm7t-x-x-120">+</span><img 
src="main18x.png" alt="1
--
2"  class="frac" align="middle"><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">Z</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup><span 
class="zptmcm7m-x-x-120">Z</span><sub>
<span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub><span 
class="zptmcm7m-x-x-120">Z</span><sup><span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sup></td>                                                        <td 
class="align-label"><a 
 id="x1-6002r9"></a>(9)
                            </td></tr><tr><td 
class="align-odd"></td>                                  <td 
class="align-even">     <span 
class="zptmcm7y-x-x-120">-</span><img 
src="main19x.png" alt="g-
2"  class="frac" align="middle"><img 
src="main20x.png" alt="(       i     )
  &#x02C9;&#x03C8; &#x03B3;&#x03BC; &#x03C4;-W i&#x03C8;
   L   2   &#x03BC; L"  class="left" align="middle"></td>                                                                <td 
class="align-label"><a 
 id="x1-6003r10"></a>(10)
                            </td></tr><tr><td 
class="align-odd"></td>                                  <td 
class="align-even">     <span 
class="zptmcm7y-x-x-120">-</span><img 
src="main21x.png" alt="  &#x2032;
g-
 2"  class="frac" align="middle"><img 
src="main22x.png" alt="(           )
 &#x02C9;&#x03C8;L&#x03B3;&#x03BC; Y&#x03C8;LB &#x03BC;"  class="left" align="middle"></td>                                                                  <td 
class="align-label"><a 
 id="x1-6004r11"></a>(11)
                            </td></tr><tr><td 
class="align-odd"></td>                                  <td 
class="align-even">     <span 
class="zptmcm7y-x-x-120">-</span><img 
src="main23x.png" alt="g-
2"  class="frac" align="middle"><img 
src="main24x.png" alt="(              )
      &#x03BC;&#x03C4;i  i
  &#x02C9;&#x03C8;R&#x03B3;  2 W &#x03BC;&#x03C8;R"  class="left" align="middle"></td>                                                                <td 
class="align-label"><a 
 id="x1-6005r12"></a>(12)
                            </td></tr><tr><td 
class="align-odd"></td>                                  <td 
class="align-even">     <span 
class="zptmcm7y-x-x-120">-</span><img 
src="main25x.png" alt="  &#x2032;
g-
 2"  class="frac" align="middle"><img 
src="main26x.png" alt="(&#x02C9;&#x03C8; &#x03B3;&#x03BC; Y&#x03C8;  B  )
  R      R &#x03BC;"  class="left" align="middle"></td>                                                                  <td 
class="align-label"><a 
 id="x1-6006r13"></a>(13)
                            </td></tr><tr><td 
class="align-odd"></td>                                  <td 
class="align-even">     <span 
class="zptmcm7t-x-x-120">+</span><img 
src="main27x.png" alt="    g
--------
2 cos&#x03B8;W"  class="frac" align="middle"><img 
src="main28x.png" alt="(      &#x03C4;i      )
  &#x03C8;&#x02C9;L &#x03B3;&#x03BC;--W i&#x03BC;&#x03C8;L
        2"  class="left" align="middle"></td>                                                          <td 
class="align-label"><a 
 id="x1-6007r14"></a>(14)
                            </td></tr><tr><td 
class="align-odd"></td>                                  <td 
class="align-even">     <span 
class="zptmcm7t-x-x-120">+</span><img 
src="main29x.png" alt="    g
--------
2 cos&#x03B8;W"  class="frac" align="middle"><img 
src="main30x.png" alt="(             )
      &#x03BC;&#x03C4;i
  &#x03C8;&#x02C9;L &#x03B3; --Z &#x03BC;&#x03C8;L
        2"  class="left" align="middle"><span 
class="zptmcm7m-x-x-120">.</span></td>                                                         <td 
class="align-label"><a 
 id="x1-6008r15"></a>(15)                            </td></tr></table>
<!--l. 24--><p class="indent" >      Where <span 
class="zptmcm7m-x-x-120">W</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;&#x03BD;</span></sub><sup><span 
class="zptmcm7m-x-x-90">i</span></sup> represents the field strength tensor for the <span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(2)</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub> gauge bosons, <span 
class="zptmcm7m-x-x-120">B</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;&#x03BD;</span></sub> is the field
strength tensor for the <span 
class="zptmcm7m-x-x-120">U</span><span 
class="zptmcm7t-x-x-120">(1)</span><sub><span 
class="zptmcm7m-x-x-90">Y</span> </sub> gauge boson, <span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">W</span> </sub> and <span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">Z</span></sub> are the masses of the <span 
class="zptmcm7m-x-x-120">W </span>and <span 
class="zptmcm7m-x-x-120">Z </span>bosons respectively,
<span 
class="zptmcm7m-x-x-120">g </span>is the <span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(2)</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub> gauge coupling constant, <span 
class="zptmcm7m-x-x-120">g</span><span 
class="zptmcm7y-x-x-120">&#x2032; </span>is the <span 
class="zptmcm7m-x-x-120">U</span><span 
class="zptmcm7t-x-x-120">(1)</span><sub><span 
class="zptmcm7m-x-x-90">Y</span> </sub> gauge coupling constant, <span 
class="zptmcm7m-x-x-120">&#x03C8;</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub> and <span 
class="zptmcm7m-x-x-120">&#x03C8;</span><sub><span 
class="zptmcm7m-x-x-90">R</span></sub>
denote the left- and right-handed fermion fields, <span 
class="zptmcm7m-x-x-120">&#x03C4;</span><sup><span 
class="zptmcm7m-x-x-90">i</span></sup> are the Pauli matrices corresponding to the
<span 
class="zptmcm7m-x-x-120">SU</span><span 
class="zptmcm7t-x-x-120">(2)</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub> symmetry, <span 
class="zptmcm7m-x-x-120">Y </span>represents the hypercharge of the fermion fields, and <span 
class="zptmcm7m-x-x-120">&#x03B8;</span><sub><span 
class="zptmcm7m-x-x-90">W</span> </sub> is the Weinberg
angle.
<!--l. 28--><p class="indent" >      The electroweak theory successfully predicted the masses of the <span 
class="zptmcm7m-x-x-120">W </span>and <span 
class="zptmcm7m-x-x-120">Z </span>bosons, which were
experimentally confirmed in 1983 at CERN. The <span 
class="zptmcm7m-x-x-120">W </span>bosons (<span 
class="zptmcm7m-x-x-120">W</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> and <span 
class="zptmcm7m-x-x-120">W</span><sup><span 
class="zptmcm7y-x-x-90">-</span></sup>) mediate charged current
interactions, while the <span 
class="zptmcm7m-x-x-120">Z </span>boson mediates neutral current interactions.
<!--l. 31--><p class="indent" >      The path to the electroweak theory was marked by significant experimental and theoretical
advances. In the 1930s, the discovery of the muon by Carl Anderson and Seth Neddermeyer introduced
the idea that there were particles beyond the electron. Muons were soon identified as heavier cousins of
                                                                                         
                                                                                         
electrons, leading to the development of the concept of lepton family.
<!--l. 35--><p class="indent" >      In the 1970s, the discovery of the tau lepton, a particle even heavier than the muon, further
expanded the lepton family. The tau lepton, discovered by Martin Perl and collaborators in
1975, was crucial in validating the electroweak theory. The existence of three generations of
leptons (electron, muon, and tau) and their associated neutrinos was essential for the theory&#8217;s
development.
<!--l. 39--><p class="indent" >      The electroweak unification also prompted the search for new particles, such as the Higgs boson,
responsible for giving mass to the gauge bosons. The discovery of the Higgs boson at the Large Hadron
Collider in 2012 was a triumph for the Standard Model and confirmed the last missing piece of the
electroweak theory.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">1.3    </span> <a 
 id="x1-70001.3"></a>neutrino</h4>
<!--l. 1--><p class="noindent" >&#x00A0;Okay!
<!--l. 4--><p class="indent" >      The stage has been set!
<!--l. 6--><p class="indent" >      Time for our stars &#8211;the neutrinos&#8211; to make an appearence.
<!--l. 8--><p class="indent" >      The story begins in the 1930s with Wolfgang Pauli, who first proposed the existence of neutrinos to
solve a pressing problem in the field of beta decay. He addressed the puzzle of the missing energy in beta
decay experiments. Beta decay is a process where a neutron decays into a proton, an electron, and an
electron antineutrino:
<!--l. 12--><p class="indent" >
      <div class="math-display" >
<img 
src="main31x.png" alt="n &#x2192; p + e- + &#x02C9;&#x03BD;e
" class="math-display" ></div>
<!--l. 16--><p class="indent" >      where <img 
src="main32x.png" alt="n " class="math"> is the neutron, <img 
src="main33x.png" alt="p " class="math"> is the proton, <img 
src="main34x.png" alt=" -
e " class="math"> is the electron, and <img 
src="main35x.png" alt="&#x02C9;&#x03BD;e  " class="math"> is the electron
antineutrino. The problem was that the energy of the emitted beta particle (electron) and the proton did not
add up to the total energy of the decaying neutron, leading to what seemed like a violation of energy
conservation.
<!--l. 19--><p class="indent" >      To deal with this, Pauli proposed the existence of a new, neutral particle that carried away the missing
energy. <span class="footnote-mark"><a 
href="main4.html#fn3x0"><sup class="textsuperscript">3</sup></a></span><a 
 id="x1-7001f3"></a> 
Fermi incorporated the neutrino into his theory of beta decay, which became known as Fermi&#8217;s theory of
beta decay. His theory elegantly explained the conservation of energy and angular momentum in beta
decay processes.
<!--l. 24--><p class="indent" >      The neutrino, denoted by <img 
src="main36x.png" alt="&#x03BD; " class="math">, is a nearly massless and electrically neutral particle. The interaction
of neutrinos is governed by the weak force
<!--l. 27--><p class="indent" >      For many years, neutrinos were a theoretical construct until they were finally observed
experimentally by Clyde Cowan and Frederick Reines in 1956. Their detection was achieved by capturing
neutrinos emitted from a nuclear reactor and observing their interactions with a detector filled with water
and cadmium chloride.
<!--l. 30--><p class="indent" >      The weak interaction is described by the exchange of W and Z bosons, which mediate processes
like beta decay.
<!--l. 32--><p class="indent" >      The 1960s introduced a new chapter with the discovery of the muon neutrino, <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub>. The experiment
conducted by the Brookhaven National Laboratory used a beam of pions, which decay into muons and
muon neutrinos:
<!--l. 35--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                      <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">&#x03C0;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> <span 
class="zptmcm7y-x-x-120">&#x2192; </span><span 
class="zptmcm7m-x-x-120">&#x03BC;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> <span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub>
<span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub></td>                                      <td 
class="align-even"></td>                                      <td 
class="align-label"><a 
 id="x1-7003r16"></a>(16)                                      </td></tr></table>
<!--l. 39--><p class="indent" >      Here, <span 
class="zptmcm7m-x-x-120">&#x03C0;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> is the positively charged pion, <span 
class="zptmcm7m-x-x-120">&#x03BC;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> is the muon, and <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub> is the muon neutrino. In 1962, the
collaboration led by Martin LPerl and his team at the Stanford Linear Accelerator Center
(SLAC) confirmed the existence of the muon neutrino by observing interactions consistent with
<span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub>.
<!--l. 42--><p class="indent" >      The next breakthrough in neutrino physics came in 1975 with the discovery of the tau neutrino, <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03C4;</span></sub>.
The relevant interaction can be expressed as:
<!--l. 45--><p class="indent" >
<table 
class="align">
                                      <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">&#x03C4;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> <span 
class="zptmcm7y-x-x-120">&#x2192; </span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub>
<span 
class="zptmcm7m-x-x-90">&#x03C4;</span></sub><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">&#x2113;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup></td>                                      <td 
class="align-even"></td>                                      <td 
class="align-label"><a 
 id="x1-7004r17"></a>(17)                                      </td></tr></table>
<!--l. 49--><p class="indent" >      where <span 
class="zptmcm7m-x-x-120">&#x03C4;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> is the positively charged tau particle, <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03C4;</span></sub> is the tau neutrino, and <span 
class="zptmcm7m-x-x-120">&#x2113;</span><sup><span 
class="zptmcm7t-x-x-90">+</span></sup> represents a lepton
                                                                                         
                                                                                         
like a positron. The detection of the tau neutrino was more challenging due to its lower production rates
and the complexity of distinguishing it from other neutrinos. These discoveries not only confirmed the
existence of the muon and tau neutrinos but also led to the realization of the three-flavor neutrino model in
the Standard Model.
      <h4 class="subsectionHead"><span class="titlemark">1.4    </span> <a 
 id="x1-80001.4"></a>Neutrino Mass Ordering</h4>
<!--l. 3--><p class="noindent" >In the early days of neutrino physics, the Standard Model treated neutrinos as massless particles but
experimentally we know that they have some amount of mass. Just a very tiny amount. Not only do
we not really know the masses of the neutrinos, the ordering of their masses is still an open
question.
<!--l. 7--><p class="indent" >      For the mass ordering of neutrinos, we need to delve into the concept of neutrino mass eigenstates
and flavor eigenstates. Neutrinos are produced and detected in flavor eigenstates (denoted by <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">e</span></sub>, <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub>, and
<span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03C4;</span></sub>), but they propagate as mass eigenstates (<span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub>, <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub>, and <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub>). The relationship between these states is
governed by the PMNS (Pontecorvo-Maki-Nakagawa-Sakata) matrix, which can be written
as:
<!--l. 11--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                            <tr><td 
class="align-odd"><img 
src="main37x.png" alt="(    )
| &#x03BD;e |
|    |
|| &#x03BD;&#x03BC; ||
(    )
  &#x03BD;&#x03C4;"  class="left" align="middle"> <span 
class="zptmcm7t-x-x-120">= </span><img 
src="main38x.png" alt="(               )
| Ue1   Ue2  Ue3|
|               |
|| U&#x03BC;1  U &#x03BC;2  U&#x03BC;3||
(               )
  U&#x03C4;1   U&#x03C4;2  U &#x03C4;3"  class="left" align="middle"><img 
src="main39x.png" alt="(   )
| &#x03BD;1|
|   |
|| &#x03BD;2||
(   )
  &#x03BD;3"  class="left" align="middle"></td>                            <td 
class="align-even"></td>                            <td 
class="align-label"><a 
 id="x1-8001r18"></a>(18)                            </td></tr></table>
<!--l. 30--><p class="indent" >      The mass eigenstates <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub>, <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub>, and <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub> have different masses, but their exact ordering is not yet
definitively known. There are two possible orderings for these masses:
<!--l. 33--><p class="indent" >      Normal Ordering (NO): In this scenario, the masses of the neutrinos are ordered as
<img 
src="main40x.png" alt="m  &#x003C;  m  &#x003C; m
  1    2     3  " class="math">. This implies that the third eigenstate, <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub>, has the highest mass. The mass differences
between these states are described by:
<!--l. 37--><p class="indent" >
<table 
class="align">
                                     <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">21</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                     <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup> <span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                                                          <td 
class="align-label"><a 
 id="x1-8002r19"></a>(19)
                                     </td></tr><tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">32</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                     <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup> <span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                                                          <td 
class="align-label"><a 
 id="x1-8003r20"></a>(20)
                                     </td></tr><tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">31</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                     <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup> <span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                                                          <td 
class="align-label"><a 
 id="x1-8004r21"></a>(21)                                     </td></tr></table>
<!--l. 43--><p class="indent" >      Inverted Ordering (IO): Here, the masses are ordered as <img 
src="main41x.png" alt="m  &#x003C; m  &#x003C;  m
  3    1    2  " class="math">. In this case, the lightest
                                                                                         
                                                                                         
eigenstate is <span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub>. The corresponding mass differences are:
<!--l. 47--><p class="indent" >
<table 
class="align">
                                     <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">21</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                     <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup> <span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                                                          <td 
class="align-label"><a 
 id="x1-8005r22"></a>(22)
                                     </td></tr><tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">32</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                     <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup> <span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                                                          <td 
class="align-label"><a 
 id="x1-8006r23"></a>(23)
                                     </td></tr><tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">31</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                     <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup> <span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                                                          <td 
class="align-label"><a 
 id="x1-8007r24"></a>(24)                                     </td></tr></table>
<!--l. 53--><p class="indent" >      Determining the correct mass ordering is essential for understanding the properties of neutrinos
and has profound implications for cosmology and particle physics.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">1.5    </span> <a 
 id="x1-90001.5"></a>Neutrino Oscillations</h4>
<!--l. 3--><p class="noindent" >Neutrino oscillation is a quantum phenomenon whereby a neutrino created with a specific lepton flavor
can change into another flavor as it propagates through space. This behavior is a direct consequence of the
fact that neutrinos have mass and the flavor eigenstates.
<!--l. 7--><p class="indent" >      The flavor states <img 
src="main42x.png" alt="&#x03BD;&#x03B1;  " class="math"> (<img 
src="main43x.png" alt="&#x03B1; = e,&#x03BC;, &#x03C4; " class="math">) are related to the mass states <img 
src="main44x.png" alt="&#x03BD;i  " class="math"> (<img 
src="main45x.png" alt="i = 1,2,3  " class="math">) through a
unitary transformation. This transformation can be expressed as:
<!--l. 10--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                    <tr><td 
class="align-odd"><span 
class="zptmcm7y-x-x-120">|</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B1;</span></sub><span 
class="zptmcm7y-x-x-120">&#x27E9;</span></td>                                    <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">U</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B1;i</span></sub><span 
class="zptmcm7y-x-x-120">|</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7y-x-x-120">&#x27E9;</span><span 
class="zptmcm7m-x-x-120">,</span></td>                                                                        <td 
class="align-label"><a 
 id="x1-9001r25"></a>(25)                                    </td></tr></table>
<!--l. 14--><p class="indent" >      where <img 
src="main46x.png" alt="U&#x03B1;i  " class="math"> are elements of the PMNS matrix, which is a unitary matrix describing the mixing
between the flavor and mass eigenstates.
<!--l. 16--><p class="indent" >      When a neutrino is produced in a flavor eigenstate, it propagates as a superposition of mass
eigenstates. If we denote the neutrino state produced at <img 
src="main47x.png" alt="t = 0  " class="math"> as <img 
src="main48x.png" alt="|&#x03BD;&#x03B1;&#x27E9; " class="math">, its time evolution in terms of the
mass eigenstates is given by:
<!--l. 19--><p class="indent" >
<table 
class="align">
                                 <tr><td 
class="align-odd"><span 
class="zptmcm7y-x-x-120">|</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B1;</span></sub><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">t</span><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7y-x-x-120">&#x27E9;</span></td>                                 <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">U</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B1;i</span></sub><span 
class="zptmcm7m-x-x-120">e</span><sup><span 
class="zptmcm7y-x-x-90">-</span><span 
class="zptmcm7m-x-x-90">iE</span><sub><span 
class="zptmcm7m-x-x-70">i</span></sub><span 
class="zptmcm7m-x-x-90">t</span></sup><span 
class="zptmcm7y-x-x-120">|</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub>
<span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7y-x-x-120">&#x27E9;</span><span 
class="zptmcm7m-x-x-120">,</span></td>                                                                  <td 
class="align-label"><a 
 id="x1-9002r26"></a>(26)                                 </td></tr></table>
<!--l. 23--><p class="indent" >      where <img 
src="main49x.png" alt="Ei  " class="math"> is the energy of the mass eigenstate <img 
src="main50x.png" alt="&#x03BD;i  " class="math">, and <img 
src="main51x.png" alt="t " class="math"> is the time of propagation.
                                                                                         
                                                                                         
<!--l. 25--><p class="indent" >      The probability of detecting a neutrino of flavor <img 
src="main52x.png" alt="&#x03B2; " class="math"> after a time <img 
src="main53x.png" alt="t " class="math"> is given by the squared
modulus of the amplitude:
<!--l. 27--><p class="indent" >
<table 
class="align">
                            <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">P</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B1;</span></sub> <span 
class="zptmcm7y-x-x-120">&#x2192; </span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B2;</span></sub><span 
class="zptmcm7t-x-x-120">;</span><span 
class="zptmcm7m-x-x-120">t</span><span 
class="zptmcm7t-x-x-120">)</span></td>                            <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><img 
src="main54x.png" alt="|          |
|&#x27E8;&#x03BD;&#x03B2;|&#x03BD;&#x03B1;(t)&#x27E9;|"  class="left" align="middle"><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                                             <td 
class="align-label"><a 
 id="x1-9003r27"></a>(27)
                            </td></tr><tr><td 
class="align-odd"></td>                                          <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><img 
src="main55x.png" alt="|              |
||              ||
|&#x2211; U *&#x03B2;iU&#x03B1;ie-iEit|
| i            |"  class="left" align="middle"><sup><span 
class="zptmcm7t-x-x-90">2</span></sup><span 
class="zptmcm7m-x-x-120">.</span></td>                                                        <td 
class="align-label"><a 
 id="x1-9004r28"></a>(28)                            </td></tr></table>
<!--l. 32--><p class="indent" >      Assuming the mass differences between the neutrino mass eigenstates are small compared to their
energies, and using the approximation <img 
src="main56x.png" alt="         m2
Ei &#x2248; E-  2iE-  " class="math">, we can simplify the expression for the
oscillation probability. The oscillation probability for a two-flavor scenario (<img 
src="main57x.png" alt="&#x03B1; " class="math"> and <img 
src="main58x.png" alt="&#x03B2; " class="math">) is given
by:
<!--l. 35--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                         <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">P</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B1;</span></sub> <span 
class="zptmcm7y-x-x-120">&#x2192; </span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">&#x03B2;</span></sub><span 
class="zptmcm7t-x-x-120">;</span><span 
class="zptmcm7m-x-x-120">L</span><span 
class="zptmcm7t-x-x-120">)</span></td>                         <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <span 
class="zptmcm7t-x-x-120">sin</span><sup><span 
class="zptmcm7t-x-x-90">2</span></sup><span 
class="zptmcm7t-x-x-120">(2</span><span 
class="zptmcm7m-x-x-120">&#x03B8;</span><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7t-x-x-120">sin</span><sup><span 
class="zptmcm7t-x-x-90">2</span></sup><img 
src="main59x.png" alt="(    2 )
  &#x0394;m--L-
   4E"  class="left" align="middle"><span 
class="zptmcm7m-x-x-120">,</span></td>                                                  <td 
class="align-label"><a 
 id="x1-9005r29"></a>(29)                         </td></tr></table>
<!--l. 39--><p class="indent" >      where <img 
src="main60x.png" alt="&#x03B8; " class="math"> is the mixing angle between the two flavors, <img 
src="main61x.png" alt="&#x0394;m2  = m2 - m2
         2    1  " class="math"> is the mass-squared
difference, <img 
src="main62x.png" alt="L " class="math"> is the distance traveled by the neutrino, and <img 
src="main63x.png" alt="E " class="math"> is the neutrino&#8217;s energy.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">1.6    </span> <a 
 id="x1-100001.6"></a>Dirac Vs MAJORANA Neutrinos</h4>
<!--l. 3--><p class="noindent" >To understand the nature of neutrino masses, it is crucial to delve into two different types of neutrinos;
Dirac and Majorana. So a new model was required to explain these discrepancies, and in comes Dirac.
Dirac, in 1928, extended his theory to include neutrinos, proposing that they were Dirac fermions.
According to Dirac&#8217;s theory, neutrinos have distinct antiparticles, and their masses arise from the Higgs
mechanism, similar to other fermions. In the Dirac framework, the Lagrangian for neutrinos can be
written as:
<!--l. 9--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                <tr><td 
class="align-odd"><span 
class="zptmcm7y-x-x-120"><img 
src="zptmcm7y-c-4c.png" alt="L" class="-120x-x-4c" /></span><sub>Dirac</sub></td>                                <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <span class="bar-css"><span 
class="zptmcm7m-x-x-120">&#x03BD;</span></span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">i&#x03B3;</span><sup><span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sup><span 
class="zptmcm7m-x-x-120">&#x2202;</span><sub>
<span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub><span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BD;</span></sub><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub><span 
class="zptmcm7m-x-x-120">,</span></td>                                                                <td 
class="align-label"><a 
 id="x1-10001r30"></a>(30)                                </td></tr></table>
<!--l. 13--><p class="indent" >      where <img 
src="main64x.png" alt="&#x03BD;L  " class="math"> denotes the left-handed neutrino field, <img 
src="main65x.png" alt="&#x03BD;&#x02C9;L  " class="math"> its Dirac adjoint, and <img 
src="main66x.png" alt="m&#x03BD;  " class="math"> is the Dirac mass
term. <img 
src="main67x.png" alt="&#x03BD;L  " class="math"> and its antiparticle <img 
src="main68x.png" alt="&#x02C9;&#x03BD;L  " class="math"> are distinct entities.
<!--l. 16--><p class="indent" >      Then comes Majorana, who proposed in 1937 a different theory to account for neutrino masses.
Majorana suggested that neutrinos could be their own antiparticles, which leads to the Majorana
condition. In this framework, neutrinos are described by Majorana fermions. The Majorana Lagrangian is
given by:
<!--l. 21--><p class="indent" >
<table 
class="align">
                             <tr><td 
class="align-odd"><span 
class="zptmcm7y-x-x-120"><img 
src="zptmcm7y-c-4c.png" alt="L" class="-120x-x-4c" /></span><sub>Majorana</sub></td>                             <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main69x.png" alt="1
--
2"  class="frac" align="middle"><span class="bar-css"><span 
class="zptmcm7m-x-x-120">&#x03BD;</span></span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub><sup><span 
class="zptmcm7m-x-x-90">c</span></sup><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">i&#x03B3;</span><sup><span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sup><span 
class="zptmcm7m-x-x-120">&#x2202;</span><sub>
<span 
class="zptmcm7m-x-x-90">&#x03BC;</span></sub><span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BD;</span></sub><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub><span 
class="zptmcm7m-x-x-120">,</span></td>                                                          <td 
class="align-label"><a 
 id="x1-10002r31"></a>(31)                             </td></tr></table>
<!--l. 25--><p class="indent" >      where <img 
src="main70x.png" alt=" c
&#x03BD;L  " class="math"> is the charge-conjugated field of <img 
src="main71x.png" alt="&#x03BD;L  " class="math">. The Majorana mass term explicitly breaks the
lepton number conservation, which is a key difference from the Dirac case. Lepton number
conservation is the principle that the total number of leptons minus antileptons remains constant in a
physical process. Majorana neutrinos are their own antiparticles, and their mass term is of the
                                                                                         
                                                                                         
form:
<!--l. 30--><p class="indent" >
<table 
class="align">
                              <tr><td 
class="align-odd"><span 
class="zptmcm7y-x-x-120"><img 
src="zptmcm7y-c-4c.png" alt="L" class="-120x-x-4c" /></span><sub>Majorana mass</sub></td>                              <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7y-x-x-120">-</span><img 
src="main72x.png" alt="1-
2"  class="frac" align="middle"><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">&#x03BD;</span></sub><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">&#x03BD;</span><sub><span 
class="zptmcm7m-x-x-90">L</span></sub><sup><span 
class="zptmcm7m-x-x-90">T</span> </sup><span 
class="zptmcm7m-x-x-120">C&#x03BD;</span><sub>
<span 
class="zptmcm7m-x-x-90">L</span></sub><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7m-x-x-120">,</span></td>                                                            <td 
class="align-label"><a 
 id="x1-10003r32"></a>(32)                              </td></tr></table>
<!--l. 34--><p class="indent" >      where <img 
src="main73x.png" alt="C " class="math"> is the charge-conjugation matrix.
<!--l. 37--><p class="indent" >      The actual nature of neutrino masses&#8212;whether Dirac or Majorana&#8212;has significant implications
for our understanding of fundamental particles and the symmetries of the universe.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-110002"></a>Neutrino Detection</h3>
<!--l. 3--><p class="noindent" >The detection of neutrinos is a crucial aspect of understanding nuclear reactions and the fundamental
structure of matter. The discovery of neutrino oscillations not only confirmed that neutrinos have mass but
also opened up new avenues for research, such as the precise determination of the neutrino
mass spectrum and the investigation of the potential for CP violation in the lepton sector.
Ongoing research aims to further understand their properties, including their masses, their role in
the universe&#8217;s matter-antimatter asymmetry, and potential new physics beyond the Standard
Model.
<!--l. 7--><p class="indent" >      In order to study them experimentally, we have to actually be able to detect them &#8211; a task made
complicated by the fact that neutrinos only interact with the weak force. Trillions of neutrinos go through
us humans every second and we don&#8217;t notice because they don&#8217;t really interact with us, instead passing
right through.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">2.1    </span> <a 
 id="x1-120002.1"></a>Types of Detectors</h4>
<!--l. 3--><p class="noindent" >Just as there are many ways to skin a cat, neutrinos can be detected using a number of detector
technologies. Each method harnesses different physical principles and technological advancements to
observe these elusive particles.
<!--l. 6--><p class="indent" >      Cherenkov detectors exploit the phenomenon of Cherenkov radiation, which occurs when a
neutrino interacts with a medium at speeds greater than the speed of light in that medium. This results in
the emission of a faint blue light, which can be detected and analyzed.
<!--l. 9--><p class="indent" >      The Super-Kamiokande detector in Japan uses a large tank filled with ultra-pure water. It contains
thousands of photomultiplier tubes that detect the Cherenkov radiation produced when neutrinos interact
with the water.
<!--l. 12--><p class="indent" >      Water Cherenkov detectors are a type of Cherenkov detector specifically utilizing water as the
                                                                                         
                                                                                         
detection medium. These detectors are characterized by their large volumes of water and arrays of
photomultiplier tubes (PMTs) arranged around the tank.
<!--l. 15--><p class="indent" >      The IceCube Neutrino Observatory at the South Pole uses a cubic-kilometer array of
detectors embedded in the Antarctic ice, capturing Cherenkov radiation from high-energy
neutrinos.
<!--l. 17--><p class="indent" >      Scintillation detectors use materials that emit light when excited by the passage of a high-energy
particle. Neutrinos interact with a scintillator material, causing it to emit flashes of light, which are then
detected by photodetectors.
<!--l. 20--><p class="indent" >      The NOVA detector utilizes a liquid scintillator to detect neutrinos over long baselines, aiding in
the study of neutrino oscillations.
<!--l. 22--><p class="indent" >      Radio detectors capture the radio waves emitted by neutrino interactions in ice or other materials.
This method is particularly useful for very high-energy neutrinos.
<!--l. 25--><p class="indent" >      The ANITA (Antarctic Impulse Transient Antenna) experiment detects high-energy neutrinos via
the radio waves produced by neutrino interactions with the Antarctic ice.
<!--l. 27--><p class="indent" >      Liquid Argon Time Projection Chambers (LArTPCs) use liquid argon as both the detector material
and the medium for drift electrons generated by neutrino interactions. The drifted electrons are then
collected and analyzed to reconstruct the interaction.
<!--l. 30--><p class="indent" >      The spatial resolution in LArTPCs depends on the drift length <img 
src="main74x.png" alt="d " class="math">, drift field <img 
src="main75x.png" alt="E " class="math">, and the electron
mobility <img 
src="main76x.png" alt="&#x03BC; " class="math">.
<!--l. 32--><p class="indent" >      The DUNE (Deep Underground Neutrino Experiment) will use LArTPCs to study neutrino
properties with high precision, particularly in the context of long-baseline neutrino oscillation
experiments.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">2.2    </span> <a 
 id="x1-130002.2"></a>Deep Underground Neutrino Experiment (DUNE)</h4>
<!--l. 3--><p class="noindent" >DUNE is a groundbreaking experiment designed to investigate neutrino properties by utilizing an
                                                                                         
                                                                                         
innovative approach involving a large detector placed deep underground. The primary goal of
DUNE is to study neutrino oscillations DUNE&#8217;s experimental setup involves a neutrino beam
generated from a high-intensity proton accelerator at Fermilab in Illinois. The beam travels
through the Earth to a massive detector located approximately 1,300 kilometers away, deep
underground at the Sanford Underground Research Facility (SURF) in South Dakota. The large
scale of the detectors allows for the precise measurement of neutrino interactions, and its
underground location minimizes interference from cosmic rays, thus improving the sensitivity of the
experiment.
<!--l. 9--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 12--><p class="noindent" ><img 
src="figures/dune.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-13001r3"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;3. </span><span  
class="content">Cartoon of the DUNE setup                                                       </span></div><!--tex4ht:label?: x1-13001r3 -->
                                                                                         
                                                                                         
<!--l. 15--><p class="indent" >      </div><hr class="endfigure">
<!--l. 17--><p class="indent" >      The plan is to have 2 sets of detectors, one at Fermilab, (near detector(ND)) and the other at SURF
(far detector (FD)). The design of the DUNE far detector, grounded in cutting-edge Liquid Argon Time
Projection Chamber (LArTPC) technology, is set to revolutionize particle physics. This detector will be
housed in a colossal volume of 70 kilotons of liquid argon, buried 1.5 kilometers underground. To
maximize the efficiency of physics experiments, the design splits this volume into four LArTPC modules,
each with a usable &#8221;fiducial volume&#8221; of 10 kilotons, avoiding interactions near the edges. To
accommodate these massive detectors, approximately 800,000 tons of rock will be excavated, creating
vast underground caverns.
<!--l. 24--><p class="indent" >      The near detector will be built on the Argon cube concept. The ND will have a modular design
combined with a novel pixellated charge readout. Previously, large detectors struggled with high
demands for drift potentials and argon purity, which often led to risks of electric breakdown and
purity losses. By breaking down a large detector into smaller, independent modules, these risks
are significantly reduced. This modularity allows for easier maintenance and more reliable
operation.
<!--l. 30--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 33--><p class="noindent" ><img 
src="figures/nd.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-13002r4"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;4. </span><span  
class="content">Design of the DUNE ND with <span 
class="zptmcm7t-x-x-120">5</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">7 </span>modules                                       </span></div><!--tex4ht:label?: x1-13002r4 -->
                                                                                         
                                                                                         
<!--l. 36--><p class="indent" >      </div><hr class="endfigure">
<!--l. 38--><p class="indent" >      The fully pixelated charge readout adds another layer of sophistication, enabling precise event
topology reconstruction. This is important for handling high-multiplicity environments where pile-up
could otherwise obscure important data. Additionally, each module captures scintillation light
to provide accurate timing information for neutrino events, further enhancing the detector&#8217;s
performance.
<!--l. 42--><p class="indent" >      The real game-changer is the scalability of this design. The modular approach means that the
detector can be expanded to accommodate a very large active mass, opening up new possibilities for
research and application.
<!--l. 45--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 48--><p class="noindent" ><img 
src="figures/ndModule.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-13003r5"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;5. </span><span  
class="content">Cutaway image of a module                                                      </span></div><!--tex4ht:label?: x1-13003r5 -->
                                                                                         
                                                                                         
<!--l. 51--><p class="indent" >      </div><hr class="endfigure">
<!--l. 53--><p class="indent" >      Because of the novelty of the technology, a scaled down prototype of the Argon cube detector
called the <span 
class="zptmcm7t-x-x-120">2</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">2 </span>has been built. Instead of having <span 
class="zptmcm7t-x-x-120">5</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">7 </span>modules, it will have<span 
class="zptmcm7t-x-x-120">2</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">2 </span>modules. Individual
modules have already been built and tested before being put together to take data as part of a
set.
<!--l. 57--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 60--><p class="noindent" ><img 
src="figures/ndPic.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-13004r6"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;6. </span><span  
class="content">Pictures of the first module to be build (Module-0)                                  </span></div><!--tex4ht:label?: x1-13004r6 -->
                                                                                         
                                                                                         
<!--l. 63--><p class="indent" >      </div><hr class="endfigure">
<!--l. 65--><p class="indent" >      The scale of DUNE and its ambitious goals are reminiscent of the dramatic shifts in scientific
paradigms brought about by the discovery of subatomic particles that challenged existing
theories.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-140003"></a>Machine Learning</h3>
<!--l. 3--><p class="noindent" >When looking at artificial intelligence (AI), everything falls on a spectrum from easily explainable to
being a black box when thinking about how the machine makes it&#8217;s decisions. On the easily explainable
side of things, we have things like decision trees.
<!--l. 6--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 8--><p class="noindent" ><img 
src="figures/decisionTree.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-14001r7"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;7. </span><span  
class="content">Example of a basic decision tree                                                  </span></div><!--tex4ht:label?: x1-14001r7 -->
                                                                                         
                                                                                         
<!--l. 11--><p class="indent" >      </div><hr class="endfigure">
<!--l. 13--><p class="indent" >      A decision tree is where we sort the data by asking a sequence of questions and following the
flowchart down to where it leads. By the time we are at the bottom of the tree and have classified the data
we can say exactly how the model does it&#8217;s classification. For instance if a decision tree is used for
mortgage decisions and the model says no, we can query and learn that it said no because you had too low
income or too low credit score for instance.
<!--l. 17--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 19--><p class="noindent" ><img 
src="figures/neuralNet1.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-14002r8"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;8. </span><span  
class="content">Example of a basic Neural Net                                                    </span></div><!--tex4ht:label?: x1-14002r8 -->
                                                                                         
                                                                                         
<!--l. 22--><p class="indent" >      </div><hr class="endfigure">
<!--l. 24--><p class="indent" >      By contrast, a machine learning model like a neural net is almost a black box with regards to how
the decisions are made. We can query the model and ask it what it made its decisions based on, however,
the features it picks out often isn&#8217;t decipherable to humans in any way. As in the previous example, if the
answer to a mortgage is no, we have no real idea why the model made that decision. That
being said, neural networks are often able to come up with better outcomes for classification
that simple models like decision trees are. In the mortgage example, even if the neural net
can&#8217;t tell us how it comes to the conclusion of approving a loan, it is still more likely to be
able to better tell who will be a good credit risk compared to the decision tree. That&#8217;s often
the trade off that we make when deciding on a more opaque model. That&#8217;s why even though
they are opaque in how they come up with their answers we still rely on them so heavily.
Because we can empirically test through monte-carlo studies how well they perform both in term
of efficiency as well as how often these models misidentify the data that we are throwing at
it.
<!--l. 33--><p class="indent" >      While a neural network is opaque about how the decisions are made, the model itself doesn&#8217;t have
to be a black box for us. We can take a peek under the hood and see how these models work. To do so, we
start up from the basic models like a perceptron and work our way to a graph neural network, finally
connecting it to how neutrino reconstruction works.
      <h4 class="subsectionHead"><span class="titlemark">3.1    </span> <a 
 id="x1-150003.1"></a>Perceptron Neuron</h4>
<!--l. 3--><p class="noindent" >A lot of things that seem incredibly easy to humans &#8211; such as recognizing the difference between say a cat
and a dog &#8211; are very difficult for computers to do. What makes it difficult to make that sort of
classification is that it is hard for humans to define concrete rules about what makes the picture of a
cat different than the picture of a dog. Neural nets approach this in a completely different
fashion.
<!--l. 7--><p class="indent" >      Instead of trying to define rules about the features that differentiate the
                                                                                         
                                                                                         
picture of a dog vs a cat, we instead classify a whole bunch of pictures by hand.
<span class="footnote-mark"><a 
href="main5.html#fn4x0"><sup class="textsuperscript">4</sup></a></span><a 
 id="x1-15001f4"></a> Then
throw those pictures at the algorithm with the correct answers and over time the computer learns to tell the
difference between that of a dog and a cat. We call an algorithm like this that separates things into two
piles a binary classifier. There are many different kinds of binary classifiers with a whole host
of advantages and disadvantages but we will start with one that is simple to understand; the
perceptron.
<!--l. 14--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 16--><p class="noindent" ><img 
src="figures/perceptron1.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-15003r9"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;9. </span><span  
class="content">Perceptron Neuron                                                               </span></div><!--tex4ht:label?: x1-15003r9 -->
                                                                                         
                                                                                         
<!--l. 19--><p class="indent" >      </div><hr class="endfigure">
<!--l. 21--><p class="indent" >      A perceptron takes a number of inputs that are binary in nature and produce a single binary output
ie.is this a dog? The figure <a 
href="#x1-15003r9">9<!--tex4ht:ref: perceptron1 --></a> has 3 inputs (<span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub>, <span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub> and <span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub>) although, more or fewer inputs may be used.
Each input then is given a weight &#8211; <span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub>, <span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub> and <span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub> in this case &#8211; and the output calculated
thus.
<!--l. 24--><p class="indent" >
<table 
class="align">
                             <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">y </span><span 
class="zptmcm7t-x-x-120">= </span><img 
src="main77x.png" alt="(
|
|||| 0 if &#x2211;i wixi &#x2264; threshhold,
|{
  1 if &#x2211; w x &#x003E; threshhold,
|||       i i i
|||
("  class="left" align="middle"></td>                             <td 
class="align-even"></td>                             <td 
class="align-label"><a 
 id="x1-15004r33"></a>(33)                             </td></tr></table>
<!--l. 31--><p class="indent" >      Used in this fashion, a perceptron can only make simple choices. Raising the threshold makes
the classification tighter while lowering it loosens the classification. Because the output of
a perceptron is binary, for more subtle distinctions, we can use the output of a perceptron
to feed into the input of the next one thus creating a network that is more able to measure
subtlety.
<!--l. 35--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 37--><p class="noindent" ><img 
src="figures/network.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-15005r10"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;10. </span><span  
class="content">Perceptron network                                                             </span></div><!--tex4ht:label?: x1-15005r10 -->
                                                                                         
                                                                                         
<!--l. 40--><p class="indent" >      </div><hr class="endfigure">
<!--l. 42--><p class="indent" >      Varying the weights of the inputs in combination with the threshold for the output allows us to get
different models of classification. The neurons in the first layer are only able to make simple decisions
based on the raw input but because we use their output as the input to the second layer, the
second layer can make more abstract decisions with a degree of subtlety impossible not only
with one perceptron but also with even a single layer of perceptrons. The complexity of the
discrimination by the classifier increasing with both the number and layers of perceptrons in the
network.
<!--l. 46--><p class="indent" >      With the correct weights and threshold values, we can get any binary classifier we want using a set
of perceptrons. That, however, puts us back at our original problem of classifying whether something is a
dog; namely, if we knew what features to look for (i.e.&#x00A0;what weights and threshold to use) it
wouldn&#8217;t be hard explaining to a computer what a dog was. The true innovation comes with using
learning algorithms that don&#8217;t require input from the programmer to set these weights and
thresholds.
<!--l. 50--><p class="indent" >      If we want to use algorithms that can adjust weights and thresholds (otherwise called
biases) automatically, we need some method where a small change in the weight only causes a
small change in the output. Because perceptrons are binary, this is impossible to do with only
perceptrons.
<!--l. 53--><p class="indent" >      A small change in the weight to an input to the perceptron can flip the output entirely. While this
small change in weight can make one of the outputs of the network better, it may also affect the rest of the
network behave in unpredictable ways. Going back to the dog and cat example, while changing the
weight slightly may make it better at recognizing dogs, it may wreak havoc on how cats are
identified.
<!--l. 57--><p class="indent" >      This is where sigmoid neurons come in.
      <h4 class="subsectionHead"><span class="titlemark">3.2    </span> <a 
 id="x1-160003.2"></a>Sigmoid Neuron</h4>
                                                                                         
                                                                                         
<!--l. 3--><p class="noindent" >While perceptrons are effectively step functions, flipping from <span 
class="zptmcm7t-x-x-120">0 </span>to <span 
class="zptmcm7t-x-x-120">1</span>, sigmoids are more smoothed out.
This means that a small change in the weight can lead to a small change in output. The sigmoid function
can be written as
<!--l. 7--><p class="indent" >
<table 
class="align">
                                       <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">&#x03C3; </span><span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main78x.png" alt="---1---
1+ e- z"  class="frac" align="middle"></td>                                       <td 
class="align-even"></td>                                       <td 
class="align-label"><a 
 id="x1-16001r34"></a>(34)                                       </td></tr></table>
<!--l. 11--><p class="indent" >      This means that a sigmoid neuron can be written as
<!--l. 13--><p class="indent" >
<table 
class="align">
                                      <tr><td 
class="align-odd"><img 
src="main79x.png" alt="      1
-------&#x2211;w-x--b
1 + e   i ii"  class="frac" align="middle"></td>                                      <td 
class="align-even"></td>                                      <td 
class="align-label"><a 
 id="x1-16002r35"></a>(35)                                      </td></tr></table>
                                                                                         
                                                                                         
<!--l. 17--><p class="indent" >      where the <span 
class="zptmcm7m-x-x-120">b </span>stands for the bias of every input. While this looks different than the perceptron at first
glance it is just a more smoothed out version of it. One key thing that we lose with the introduction of
sigmoids is the linearity that perceptrons afforded us. What we gain is the ability for our programs to
automatically adjust their weights and biases because a small change in weights does lead to small change
in output as shown in equation <a 
href="#x1-16003r36">36<!--tex4ht:ref: bias --></a>.
<!--l. 22--><p class="indent" >
<table 
class="align">
                                 <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">y </span><span 
class="zptmcm7y-x-x-120">&#x2248;</span><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><img 
src="main80x.png" alt="-&#x2202;y-
&#x2202;wi"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><img 
src="main81x.png" alt="&#x2202;y-
&#x2202;b"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">b</span></td>                                 <td 
class="align-even"></td>                                 <td 
class="align-label"><a 
 id="x1-16003r36"></a>(36)                                 </td></tr></table>
<!--l. 26--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 29--><p class="noindent" ><img 
src="figures/sigmoid.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-16004r11"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;11. </span><span  
class="content">Sigmoid Function                                                              </span></div><!--tex4ht:label?: x1-16004r11 -->
                                                                                         
                                                                                         
<!--l. 32--><p class="indent" >      </div><hr class="endfigure">
<!--l. 34--><p class="indent" >      More than the exact formula of the sigmoid neuron what matters is the shape. As a result, other
neurons can be used in it&#8217;s stead which retain the property of having a small change in weight lead to a
small change in output. Some of the more popular of these functions (called activation functions) are
RELU and softmax. Each have their own advantages and disadvantages and may even be mixed in the
same neural network
      <h4 class="subsectionHead"><span class="titlemark">3.3    </span> <a 
 id="x1-170003.3"></a>Activation Functions</h4>
<!--l. 3--><p class="noindent" >One drawback of the sigmoid function is that its gradients can become very small for large positive or
negative inputs, leading to the vanishing gradient problem during backpropagation. Backpropagation is an
optimization algorithm used to minimize the error of neural networks by calculating the gradient of the
loss function with respect to each weight through the chain rule and updating the weights
accordingly.
<!--l. 6--><p class="indent" >      The hyperbolic tangent function is another activation function that provides output values between
-1 and 1. It is defined as:
<!--l. 9--><p class="indent" >
<table 
class="align">
                                    <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">tanh</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">z</span><span 
class="zptmcm7t-x-x-120">) =</span> <img 
src="main82x.png" alt=" z   - z
e---e---
ez+ e- z"  class="frac" align="middle"></td>                                    <td 
class="align-even"></td>                                    <td 
class="align-label"><a 
 id="x1-17001r37"></a>(37)                                    </td></tr></table>
                                                                                         
                                                                                         
<!--l. 13--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 16--><p class="noindent" ><img 
src="figures/tanh.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-17002r12"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;12. </span><span  
class="content">tanh Function                                                                  </span></div><!--tex4ht:label?: x1-17002r12 -->
                                                                                         
                                                                                         
<!--l. 19--><p class="indent" >      </div><hr class="endfigure">
<!--l. 21--><p class="indent" >      In a neural network, a neuron with the tanh activation function computes:
<!--l. 23--><p class="indent" >
<table 
class="align">
                        <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">tanh</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">b</span><span 
class="zptmcm7t-x-x-120">) =</span> <img 
src="main83x.png" alt=" (&#x2211;iwixi+b)    -(&#x2211;iwixi+b)
e-----------e----------
e(&#x2211;iwixi+b)+  e-(&#x2211;iwixi+b)"  class="frac" align="middle"></td>                        <td 
class="align-even"></td>                        <td 
class="align-label"><a 
 id="x1-17003r38"></a>(38)                        </td></tr></table>
<!--l. 27--><p class="indent" >      The tanh function is zero-centered, which helps in making the learning process faster and more
efficient compared to the sigmoid function. It also suffers from the vanishing gradient problem, though it
generally performs better in practice than sigmoid.
<!--l. 30--><p class="indent" >      The Rectified Linear Unit (ReLU) is a widely used activation function in deep learning models. It
is defined as:
<!--l. 33--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                   <tr><td 
class="align-odd">ReLU<span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">z</span><span 
class="zptmcm7t-x-x-120">) =</span> <span 
class="zptmcm7t-x-x-120">max</span><span 
class="zptmcm7t-x-x-120">(0</span><span 
class="zptmcm7m-x-x-120">,z</span><span 
class="zptmcm7t-x-x-120">)</span></td>                                   <td 
class="align-even"></td>                                   <td 
class="align-label"><a 
 id="x1-17004r39"></a>(39)                                   </td></tr></table>
<!--l. 37--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 40--><p class="noindent" ><img 
src="figures/relu.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-17005r13"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;13. </span><span  
class="content">RELU function                                                                 </span></div><!--tex4ht:label?: x1-17005r13 -->
                                                                                         
                                                                                         
<!--l. 43--><p class="indent" >      </div><hr class="endfigure">
<!--l. 45--><p class="indent" >      For a neuron using ReLU, the output is computed as:
<!--l. 47--><p class="indent" >
<table 
class="align">
                          <tr><td 
class="align-odd">ReLU<span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">b</span><span 
class="zptmcm7t-x-x-120">) =</span> <span 
class="zptmcm7t-x-x-120">max</span><span 
class="zptmcm7t-x-x-120">(0</span><span 
class="zptmcm7m-x-x-120">,</span><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">b</span><span 
class="zptmcm7t-x-x-120">)</span></td>                          <td 
class="align-even"></td>                          <td 
class="align-label"><a 
 id="x1-17006r40"></a>(40)                          </td></tr></table>
<!--l. 51--><p class="indent" >      ReLU introduces non-linearity while being computationally efficient. It helps mitigate the
vanishing gradient problem by allowing gradients to flow more easily through the network. However, it
suffers from the &#8221;dying ReLU&#8221; problem where neurons can sometimes become inactive and only output
zero.
<!--l. 55--><p class="indent" >      To address the dying ReLU problem, the Leaky ReLU function introduces a small, non-zero
gradient for negative inputs. It is defined as:
<!--l. 58--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                             <tr><td 
class="align-odd">Leaky ReLU<span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">z</span><span 
class="zptmcm7t-x-x-120">) = </span><img 
src="main84x.png" alt="(|
|{ z    if z &#x003E; 0

||(
  &#x03B1;z   if z &#x2264; 0"  class="left" align="middle"></td>                             <td 
class="align-even"></td>                             <td 
class="align-label"><a 
 id="x1-17007r41"></a>(41)                             </td></tr></table>
<!--l. 65--><p class="indent" >      where <img 
src="main85x.png" alt="&#x03B1; " class="math"> is a small constant (e.g., 0.01).
<!--l. 67--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 70--><p class="noindent" ><img 
src="figures/lrelu.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-17008r14"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;14. </span><span  
class="content">Leaky RELU function                                                           </span></div><!--tex4ht:label?: x1-17008r14 -->
                                                                                         
                                                                                         
<!--l. 73--><p class="indent" >      </div><hr class="endfigure">
<!--l. 74--><p class="indent" >      For a neuron using Leaky ReLU, the output is:
<!--l. 76--><p class="indent" >
<table 
class="align">
               <tr><td 
class="align-odd">Leaky ReLU<span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">b</span><span 
class="zptmcm7t-x-x-120">) = </span><img 
src="main86x.png" alt="(
|
|{ &#x2211;i wixi+ b      if &#x2211;iwixi+ b &#x003E; 0

||( &#x03B1; (&#x2211; w x + b)   if &#x2211; w x + b &#x2264; 0
      i  ii          i  ii"  class="left" align="middle"></td>               <td 
class="align-even"></td>               <td 
class="align-label"><a 
 id="x1-17009r42"></a>(42)               </td></tr></table>
<!--l. 83--><p class="indent" >      The Exponential Linear Unit (ELU) is designed to combine the benefits of ReLU and Leaky ReLU
while addressing their limitations. It is defined as:
<!--l. 86--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                              <tr><td 
class="align-odd">ELU<span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">z</span><span 
class="zptmcm7t-x-x-120">) = </span><img 
src="main87x.png" alt="(|
|{ z          if z &#x003E; 0

||(     z
  &#x03B1; (e - 1)  if z &#x2264; 0"  class="left" align="middle"></td>                              <td 
class="align-even"></td>                              <td 
class="align-label"><a 
 id="x1-17010r43"></a>(43)                              </td></tr></table>
<!--l. 93--><p class="indent" >      where <img 
src="main88x.png" alt="&#x03B1; " class="math"> is a positive constant.
<!--l. 95--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 99--><p class="noindent" ><img 
src="figures/elu.png" alt="PIC"  
width="227" height="227" > <a 
 id="x1-17011r15"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;15. </span><span  
class="content">ELU function                                                                  </span></div><!--tex4ht:label?: x1-17011r15 -->
                                                                                         
                                                                                         
<!--l. 102--><p class="indent" >      </div><hr class="endfigure">
<!--l. 103--><p class="indent" >      For a neuron using ELU, the output is:
<!--l. 105--><p class="indent" >
<table 
class="align">
                 <tr><td 
class="align-odd">ELU<span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">b</span><span 
class="zptmcm7t-x-x-120">) = </span><img 
src="main89x.png" alt="(
|
|{ &#x2211;iwixi+ b          if &#x2211;iwixi+ b &#x003E; 0

||( &#x03B1; (e(&#x2211;iwixi+b)- 1)   if &#x2211; w x + b &#x2264; 0
                        i  ii"  class="left" align="middle"></td>                 <td 
class="align-even"></td>                 <td 
class="align-label"><a 
 id="x1-17012r44"></a>(44)                 </td></tr></table>
<!--l. 112--><p class="indent" >      ELU can help speed up learning and improve robustness to noise by reducing the impact of
vanishing gradients.
      <h4 class="subsectionHead"><span class="titlemark">3.4    </span> <a 
 id="x1-180003.4"></a>Neural Network</h4>
<!--l. 3--><p class="noindent" >A number of these sigmoid neurons (or neurons with other activation functions) can be strung together to
make a neural network. Each neural network has 3 main parts.
<!--l. 6--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 8--><p class="noindent" ><img 
src="figures/neuralNet1.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-18001r16"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;16. </span><span  
class="content">Parts of a Neural Net                                                            </span></div><!--tex4ht:label?: x1-18001r16 -->
                                                                                         
                                                                                         
<!--l. 11--><p class="indent" >      </div><hr class="endfigure">
<!--l. 13--><p class="indent" >      First, we have an input layer. This is all the inputs that go into a neural network and is
usually represented as a vector. Each input adds one to the dimension of the input vector. Even
something like a 2d picture can have its rows stitched together to make one long vector of
inputs.
<!--l. 18--><p class="indent" >      The middle bits are called the hidden layer, not for any profound reason, but just to distinguish
them from the input and output layers. You can have as many hidden middle layers as you want in the
network. The trade off is usually one of efficiency and accuracy. The more hidden layers you have, the
more accurate the output will bebut at the cost of requiring more time to train because there are more
weights to get right. After a point, adding more layers does not improve accuracy in meaningful way
while still taking longer to train. This makes creating a good neural net less of a hard science and more of
an art form.
<!--l. 25--><p class="indent" >      Finally, we have the output layer. This layer usually has one neuron for each thing the classifier can
bin the input into. In the dog and cat case, we would have <span 
class="zptmcm7t-x-x-120">2 </span>output neurons, one that signifies dog and the
other cat. However, the neurons won&#8217;t directly tell us whether the picture contains a dog or a cat but rather
give us two values. One of these values indicates how likely it is for this picture to contain a cat and the
other represents the likelyhood that the picture contains a dog. After that, it is still up to us to decide
on cutoff values to determine whether we will say the picture contains a cat, a dog, both or
neither.
      <h4 class="subsectionHead"><span class="titlemark">3.5    </span> <a 
 id="x1-190003.5"></a>Gradient Descent</h4>
<!--l. 3--><p class="noindent" >So far we&#8217;ve talked about the fact that weights and biases can be adjusted and that it only
works if a small change creates only a small change in output while glossing over how
exactly the computer automatically calculates these weights. Time to peel back that layer!
                                                                                         
                                                                                         
<span class="footnote-mark"><a 
href="main6.html#fn5x0"><sup class="textsuperscript">5</sup></a></span><a 
 id="x1-19001f5"></a> We
use a technique called gradient descent.
<!--l. 8--><p class="indent" >      To start off, we need a set of inputs <span 
class="zptmcm7m-x-x-120">x </span>where we already know the answers <span 
class="zptmcm7m-x-x-120">y</span>. This is called the
training dataset. Once the weights and biases are adjusted we can then use the model to query a set of
inputs that we don&#8217;t know and be reasonably certain that it won&#8217;t give us garbage outputs. To do this
adjustment, we need to define a cost function.
<!--l. 13--><p class="indent" >
<table 
class="align">
                                <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">C</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">w,b</span><span 
class="zptmcm7t-x-x-120">) =</span> <img 
src="main90x.png" alt="1--
2n"  class="frac" align="middle"><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">x</span></sub><span 
class="zptmcm7y-x-x-120">||</span><span 
class="zptmcm7m-x-x-120">y</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">x</span><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">a</span><span 
class="zptmcm7y-x-x-120">||</span><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                <td 
class="align-even"></td>                                <td 
class="align-label"><a 
 id="x1-19003r45"></a>(45)                                </td></tr></table>
<!--l. 17--><p class="indent" >      Where <span 
class="zptmcm7m-x-x-120">n </span>is the number of training samples and <span 
class="zptmcm7m-x-x-120">a </span>is the vector of outputs from the network. We
want a set of weights that make the cost as small as possible and we can do that through a method called
gradient descent. The function described here is not the only cost function possible but is a simple one to
start with. To use gradient descent, we can do
<!--l. 22--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                       <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">v </span><span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">&#x03B7;</span><span 
class="zptmcm7y-x-x-120">&#x2207;</span><span 
class="zptmcm7m-x-x-120">C</span></td>                                       <td 
class="align-even"></td>                                       <td 
class="align-label"><a 
 id="x1-19004r46"></a>(46)                                       </td></tr></table>
<!--l. 26--><p class="indent" >      where <span 
class="zptmcm7m-x-x-120">v </span>is the set of weights and biases and <span 
class="zptmcm7m-x-x-120">&#x03B7; </span>is the learning rate. The more aggressive we set <span 
class="zptmcm7m-x-x-120">&#x03B7;</span>
the quicker training will go, but it may end up actually increasing the cost function. So we want an <span 
class="zptmcm7m-x-x-120">&#x03B7; </span>that
is small but not too small.
      <h4 class="subsectionHead"><span class="titlemark">3.6    </span> <a 
 id="x1-200003.6"></a>Convolutional Neural Networks</h4>
<!--l. 3--><p class="noindent" >Convolutional Neural Networks (CNNs) are a specialized class of neural networks designed for
processing data with grid-like topology, such as images. They are particularly effective for
image classification and object detection due to their ability to capture spatial hierarchies in
data. A CNN typically consists of several key layers: convolutional layers, activation layers,
pooling layers, and fully connected layers. Let&#8217;s break down each layer and its role in the
network.
<!--l. 8--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 11--><p class="noindent" ><img 
src="figures/cnn.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-20001r17"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;17. </span><span  
class="content">Cartoon depicting layers of a CNN                                               </span></div><!--tex4ht:label?: x1-20001r17 -->
                                                                                         
                                                                                         
<!--l. 14--><p class="indent" >      </div><hr class="endfigure">
<!--l. 16--><p class="indent" >      Convolutional Layer
<!--l. 18--><p class="indent" >      The convolutional layer is the cornerstone of a CNN. It applies a set of filters (or kernels) to the
input image to produce feature maps. Each filter is a small matrix that slides over the input image to
compute a dot product.
<!--l. 22--><p class="indent" >      Given an input image <img 
src="main91x.png" alt="I " class="math"> of size <img 
src="main92x.png" alt="H &#x00D7; W " class="math"> (height <img 
src="main93x.png" alt="H " class="math"> and width <img 
src="main94x.png" alt="W " class="math">) and a filter <img 
src="main95x.png" alt="K " class="math"> of size
<img 
src="main96x.png" alt="kh &#x00D7; kw  " class="math"> (height <img 
src="main97x.png" alt="kh  " class="math"> and width <img 
src="main98x.png" alt="kw  " class="math">), the output feature map <img 
src="main99x.png" alt="O " class="math"> can be computed using the convolution
operation:
<!--l. 24--><p class="indent" >
<table 
class="align">
                          <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">O</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">i,j</span><span 
class="zptmcm7t-x-x-120">)</span></td>                          <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">m</span><span 
class="zptmcm7t-x-x-90">=0</span></sub><sup><span 
class="zptmcm7m-x-x-90">k</span><sub><span 
class="zptmcm7m-x-x-70">h</span></sub><span 
class="zptmcm7y-x-x-90">-</span><span 
class="zptmcm7t-x-x-90">1</span></sup><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub>
<span 
class="zptmcm7m-x-x-90">n</span><span 
class="zptmcm7t-x-x-90">=0</span></sub><sup><span 
class="zptmcm7m-x-x-90">k</span><sub><span 
class="zptmcm7m-x-x-70">w</span></sub><span 
class="zptmcm7y-x-x-90">-</span><span 
class="zptmcm7t-x-x-90">1</span></sup><span 
class="zptmcm7m-x-x-120">I</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">i</span><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">m,j</span><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">n</span><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7y-x-x-120">&#x22C5;</span><span 
class="zptmcm7m-x-x-120">K</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">m,n</span><span 
class="zptmcm7t-x-x-120">)</span></td>                                                    <td 
class="align-label"><a 
 id="x1-20002r47"></a>(47)
                          </td></tr><tr><td 
class="align-odd"></td>                                <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= (</span><span 
class="zptmcm7m-x-x-120">I </span><span 
class="zptmcm7y-x-x-120">*</span><span 
class="zptmcm7m-x-x-120">K</span><span 
class="zptmcm7t-x-x-120">)(</span><span 
class="zptmcm7m-x-x-120">i,j</span><span 
class="zptmcm7t-x-x-120">)</span></td>                                                                                  <td 
class="align-label"><a 
 id="x1-20003r48"></a>(48)                          </td></tr></table>
<!--l. 29--><p class="indent" >      where <img 
src="main100x.png" alt="* " class="math"> denotes the convolution operation. The dimensions of the output feature map depend
on the stride <img 
src="main101x.png" alt="s " class="math"> and padding <img 
src="main102x.png" alt="p " class="math">. If we use zero-padding and stride <img 
src="main103x.png" alt="s = 1  " class="math">, the dimensions
are:
<!--l. 33--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                  <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">H</span><sub><span 
class="zptmcm7m-x-x-90">out</span></sub></td>                                  <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main104x.png" alt="H---kh+-2p--
     s"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">+1</span></td>                                                                    <td 
class="align-label"><a 
 id="x1-20004r49"></a>(49)
                                  </td></tr><tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">W</span><sub><span 
class="zptmcm7m-x-x-90">out</span></sub></td>                                  <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main105x.png" alt="W---kw-+-2p-
     s"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">+1</span></td>                                                                    <td 
class="align-label"><a 
 id="x1-20005r50"></a>(50)                                  </td></tr></table>
<!--l. 38--><p class="indent" >      Activation Layer
<!--l. 40--><p class="indent" >      The Rectified Linear Unit (ReLU) is one of the most commonly used activation functions in
CNNs.
<!--l. 42--><p class="indent" >      This activation function helps in mitigating the vanishing gradient problem and speeds up
training.
<!--l. 44--><p class="indent" >      Pooling Layer
<!--l. 46--><p class="indent" >      The pooling layer reduces the spatial dimensions of the feature map, which helps in reducing
computational complexity and preventing overfitting. The most common pooling operation is max
pooling. For a pooling window of size <img 
src="main106x.png" alt="ph&#x00D7; pw  " class="math"> and stride <img 
src="main107x.png" alt="s " class="math">, the max pooling operation is defined
as:
<!--l. 50--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                         <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">O</span><sub><span 
class="zptmcm7m-x-x-90">pool</span></sub><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">i,j</span><span 
class="zptmcm7t-x-x-120">)</span></td>                         <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <span 
class="zptmcm7t-x-x-120">max</span><sub><span 
class="zptmcm7m-x-x-90">m</span><span 
class="zptmcm7y-x-x-90">&#x2208;</span><span 
class="zptmcm7t-x-x-90">[</span><span 
class="zptmcm7m-x-x-90">i</span><span 
class="zptmcm7t-x-x-90">:</span><span 
class="zptmcm7m-x-x-90">i</span><span 
class="zptmcm7t-x-x-90">+</span><span 
class="zptmcm7m-x-x-90">p</span><sub><span 
class="zptmcm7m-x-x-70">h</span></sub><span 
class="zptmcm7t-x-x-90">]</span><span 
class="zptmcm7m-x-x-90">,n</span><span 
class="zptmcm7y-x-x-90">&#x2208;</span><span 
class="zptmcm7t-x-x-90">[</span><span 
class="zptmcm7m-x-x-90">j</span><span 
class="zptmcm7t-x-x-90">:</span><span 
class="zptmcm7m-x-x-90">j</span><span 
class="zptmcm7t-x-x-90">+</span><span 
class="zptmcm7m-x-x-90">p</span><sub><span 
class="zptmcm7m-x-x-70">w</span></sub><span 
class="zptmcm7t-x-x-90">]</span></sub><span 
class="zptmcm7m-x-x-120">O</span><sub><span 
class="zptmcm7m-x-x-90">ReLU</span></sub><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">m,n</span><span 
class="zptmcm7t-x-x-120">)</span></td>                                                  <td 
class="align-label"><a 
 id="x1-20006r51"></a>(51)                         </td></tr></table>
<!--l. 54--><p class="indent" >      where <img 
src="main108x.png" alt="OReLU  " class="math"> is the feature map after applying the ReLU activation. Pooling reduces the
dimensions of the feature map:
<!--l. 57--><p class="indent" >
<table 
class="align">
                                    <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">H</span><sub><span 
class="zptmcm7m-x-x-90">out</span></sub></td>                                    <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main109x.png" alt="H -  p
------h
   s"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">+1</span></td>                                                                        <td 
class="align-label"><a 
 id="x1-20007r52"></a>(52)
                                    </td></tr><tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">W</span><sub><span 
class="zptmcm7m-x-x-90">out</span></sub></td>                                    <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main110x.png" alt="W----pw-
   s"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">+1</span></td>                                                                        <td 
class="align-label"><a 
 id="x1-20008r53"></a>(53)                                    </td></tr></table>
<!--l. 62--><p class="indent" >      Fully Connected Layer
<!--l. 64--><p class="indent" >      The fully connected layer (FC layer) is typically used at the end of the network to produce the final
classification results. It connects every neuron in the previous layer to every neuron in the current layer.
The output of a fully connected layer is computed as:
<!--l. 68--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                     <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">z</span><sub><span 
class="zptmcm7m-x-x-90">j</span></sub></td>                                     <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span><span 
class="zptmcm7t-x-x-90">=1</span></sub><sup><span 
class="zptmcm7m-x-x-90">N</span></sup><span 
class="zptmcm7m-x-x-120">w</span><sub>
<span 
class="zptmcm7m-x-x-90">ij</span></sub><span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><span 
class="zptmcm7m-x-x-120">b</span><sub><span 
class="zptmcm7m-x-x-90">j</span></sub></td>                                                                          <td 
class="align-label"><a 
 id="x1-20009r54"></a>(54)                                     </td></tr></table>
<!--l. 72--><p class="indent" >      where <img 
src="main111x.png" alt="wij  " class="math"> are the weights, <img 
src="main112x.png" alt="xi  " class="math"> are the inputs from the previous layer, and <img 
src="main113x.png" alt="bj  " class="math"> is the bias. This
results in a vector of size equal to the number of classes, which can be fed into a softmax function for
classification:
<!--l. 75--><p class="indent" >
<table 
class="align">
                                   <tr><td 
class="align-odd">Softmax<span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">z</span><sub><span 
class="zptmcm7m-x-x-90">j</span></sub><span 
class="zptmcm7t-x-x-120">)</span></td>                                   <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main114x.png" alt="   zj
--e---
&#x2211;k ezk"  class="frac" align="middle"></td>                                                                      <td 
class="align-label"><a 
 id="x1-20010r55"></a>(55)                                   </td></tr></table>
<!--l. 79--><p class="indent" >      These layers work together to learn hierarchical features from raw data, making CNNs highly
effective for various image processing tasks.
                                                                                         
                                                                                         
      <h4 class="subsectionHead"><span class="titlemark">3.7    </span> <a 
 id="x1-210003.7"></a>Graph Neural Networks</h4>
<!--l. 3--><p class="noindent" >Graph Neural Networks (GNNs) extend neural network methodologies to handle data represented in the
form of graphs. GNNs are designed to work with the complex, non-Euclidean structure of graphs. Graphs
consist of nodes (or vertices) and edges (connections between nodes), and GNNs leverage these structures
to learn representations of nodes and their relationships.
<!--l. 7--><p class="indent" >      To understand how GNNs function, it&#8217;s important to break down their key components: node
features, edge features, and the message-passing mechanism.
<!--l. 9--><p class="indent" >      In a graph, each node can have associated features, which are typically represented as vectors. If
we denote the feature vector of node <img 
src="main115x.png" alt="v " class="math"> as <img 
src="main116x.png" alt="hv  " class="math">, then the node features for all nodes in the
graph can be organized into a matrix <img 
src="main117x.png" alt="H  " class="math">, where each row corresponds to a node&#8217;s feature
vector.
<!--l. 12--><p class="indent" >      Similarly, edges in a graph can also have features. For an edge connecting nodes <img 
src="main118x.png" alt="u " class="math"> and <img 
src="main119x.png" alt="v " class="math">, we
denote the edge features as <img 
src="main120x.png" alt="euv  " class="math">. These features can be organized into an edge feature matrix
<img 
src="main121x.png" alt="E  " class="math">.
<!--l. 16--><p class="indent" >      The core idea of GNNs is the message-passing mechanism, which allows nodes to aggregate
information from their neighbors. This process involves the following steps:
<!--l. 19--><p class="indent" >
      <ol  class="enumerate1" >
<li 
  class="enumerate" id="x1-21002x1">
      <!--l. 21--><p class="noindent" >Message Computation: For each node <img 
src="main122x.png" alt="v " class="math">, we compute messages from its neighboring nodes
      <img 
src="main123x.png" alt="N  (v)  " class="math">. The message <img 
src="main124x.png" alt="muv  " class="math"> from node <img 
src="main125x.png" alt="u " class="math"> to node <img 
src="main126x.png" alt="v " class="math"> is typically computed using a function
      <img 
src="main127x.png" alt="&#x03D5; " class="math">, which can depend on the features of both nodes and the edge between them:
      <!--l. 25--><p class="noindent" >
                                                                                         
                                                                                         
      <table 
class="align">
                                                         <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">uv</span></sub></td>                                                <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">&#x03D5;</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="ptmb7t-x-x-120">h</span><sub><span 
class="zptmcm7m-x-x-90">u</span></sub><span 
class="zptmcm7m-x-x-120">,</span><span 
class="ptmb7t-x-x-120">h</span><sub><span 
class="zptmcm7m-x-x-90">v</span></sub><span 
class="zptmcm7m-x-x-120">,</span><span 
class="ptmb7t-x-x-120">e</span><sub><span 
class="zptmcm7m-x-x-90">uv</span></sub><span 
class="zptmcm7t-x-x-120">)</span></td>                                                                                      <td 
class="align-label"><a 
 id="x1-21003r56"></a>(56)
                                                         </td></tr><tr><td 
class="align-odd"></td>                                                               <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">&#x03D5;</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="ptmb7t-x-x-120">h</span><sub><span 
class="zptmcm7m-x-x-90">u</span></sub><span 
class="zptmcm7m-x-x-120">,</span><span 
class="ptmb7t-x-x-120">e</span><sub><span 
class="zptmcm7m-x-x-90">uv</span></sub><span 
class="zptmcm7t-x-x-120">)</span></td>                                                                                          <td 
class="align-label"><a 
 id="x1-21004r57"></a>(57)                                                         </td></tr></table>
      </li>
<li 
  class="enumerate" id="x1-21006x2">
      <!--l. 30--><p class="noindent" >Aggregation: After computing the messages, each node aggregates the messages from its
      neighbors. The aggregation function <img 
src="main128x.png" alt="AG G  " class="math"> could be a sum, mean, or a more complex
      operation:
      <!--l. 34--><p class="noindent" >
      <table 
class="align">
                                                        <tr><td 
class="align-odd"><span 
class="ptmb7t-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">v</span></sub></td>                                               <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> AGG<sub><span 
class="zptmcm7m-x-x-90">u</span><span 
class="zptmcm7y-x-x-90">&#x2208;<img 
src="zptmcm7y-9-4e.png" alt="N" class="-90x-x-4e" /> </span><span 
class="zptmcm7t-x-x-90">(</span><span 
class="zptmcm7m-x-x-90">v</span><span 
class="zptmcm7t-x-x-90">)</span></sub><span 
class="zptmcm7m-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">uv</span></sub></td>                                                                                              <td 
class="align-label"><a 
 id="x1-21007r58"></a>(58)                                                        </td></tr></table>
                                                                                         
                                                                                         
      </li>
<li 
  class="enumerate" id="x1-21009x3">
      <!--l. 38--><p class="noindent" >Update: The aggregated message is then used to update the node&#8217;s feature vector. This update
      function <img 
src="main129x.png" alt="U PD ATE  " class="math"> often involves a neural network layer like a fully connected layer or a more
      complex function:
      <!--l. 42--><p class="noindent" >
      <table 
class="align">
                                                       <tr><td 
class="align-odd"><span 
class="ptmb7t-x-x-120">h</span><sub><span 
class="zptmcm7m-x-x-90">v</span></sub><span 
class="zptmcm7y-x-x-120">&#x2032;</span></td>                  <td 
class="align-even"> <span 
class="zptmcm7t-x-x-120">=</span> UPDATE<span 
class="zptmcm7t-x-x-120">(</span><span 
class="ptmb7t-x-x-120">h</span><sub><span 
class="zptmcm7m-x-x-90">v</span></sub><span 
class="zptmcm7m-x-x-120">,</span><span 
class="ptmb7t-x-x-120">m</span><sub><span 
class="zptmcm7m-x-x-90">v</span></sub><span 
class="zptmcm7t-x-x-120">)</span></td>                                                                                  <td 
class="align-label"><a 
 id="x1-21010r59"></a>(59)                                                       </td></tr></table>
      <!--l. 46--><p class="noindent" >The updated feature vector <img 
src="main130x.png" alt=" &#x2032;
hv  " class="math"> represents the new state of the node after considering its
      neighbors.
      <!--l. 48--><p class="noindent" >Different GNN architectures can use various choices for the aggregation and update
      functions.
      </li></ol>
<!--l. 52--><p class="indent" >      Graph Neural Networks allow for the processing of graph-structured data by iteratively updating
node features through message passing. This approach allows GNNs to capture complex relationships
between nodes and learn meaningful representations that are useful for various tasks such as node
classification and graph classification.
                                                                                         
                                                                                         
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">3.8    </span> <a 
 id="x1-220003.8"></a>Model Development</h4>
<!--l. 3--><p class="noindent" >Developing a neural net isn&#8217;t just about figuring out the neurons for the network and adjusting the weights.
The task of making a neural net can be broken up into 3 main parts.
<!--l. 6--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 8--><p class="noindent" ><img 
src="figures/mlLifecycle.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-22001r18"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;18. </span><span  
class="content">Model development                                                             </span></div><!--tex4ht:label?: x1-22001r18 -->
                                                                                         
                                                                                         
<!--l. 11--><p class="indent" >      </div><hr class="endfigure">
<!--l. 13--><p class="indent" >      The first step of any kind of model development is looking at both what kind of data is available as
well as what kind of input we might want to make on the model. The data may be scattered about in many
places and often will require processing before it can be vectorized.
<!--l. 16--><p class="indent" >      In the context of neutrino reconstruction, this may require running monte carlo simulations with
standard software e.g. (LArSoft, NDsim) and then taking the output from those simulations,
processing it into standard images that libraries like pytorch or tensorflow can take as input. It
is also important to think about standardizing the size of those images and thinking about
how to toss out the sheer amount of data that has no hits in it because neutrino events are so
sparse.
<!--l. 19--><p class="indent" >      Once that has been done, we can look at actually implementing a neural network based on that
data. This involves setting out training pipelines which will determine how the data flows, as well as
figuring out the structure of the network that will be made. Tests also have to be written for the network so
that it can be deployed robustly. Once the training with the training dataset is complete, the model has to
be validated with a validation dataset. The validation set will also be a set where the answers are
previously known so we can see how well the model performs on data that it hasn&#8217;t previously been run
on.
<!--l. 25--><p class="indent" >      Once the model has been validated, it can finally be deployed for real world data where we don&#8217;t
have the answers. This is the inference part of the model lifecycle.
      <h4 class="subsectionHead"><span class="titlemark">3.9    </span> <a 
 id="x1-230003.9"></a>Model Optimization</h4>
<!--l. 3--><p class="noindent" >There are two parts where a model can be optimized. The first is the training phase. Models can take a
long time to train even if a lot of data is available which means it is often worth it to optimize the training
phase. This sort of optimization is called hyperparameter optimization because the actual hyperparameters
(weights and biases) aren&#8217;t being tweaked but rather the parameters that guide how they are formed. It
involves manipulating the structure of the network as well as changing factors such as the learning rate.
                                                                                         
                                                                                         
The difference between a naive implementation and an optimized one may lead to a speedup of hours for
the training.
<!--l. 10--><p class="indent" >      Training isn&#8217;t however what a network is spending most of its time doing. Most of the time a
network is used to query for answers, i.e.&#x00A0;inference. Inference speedups can be done through
a number of ways such as using more specialized hardware like FPGA&#8217;s or working with
TensorRT optimization. That can bring down the time it takes to query the model for information
which can vastly affect number of events being processed in any time period thus increasing
throughput.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">4    </span> <a 
 id="x1-240004"></a>Reconstruction using SPINE</h3>
<!--l. 3--><p class="noindent" >When working with LArTPC detectors, fundamentally, all it returns is a series of voltages
that have been read out to the readout chips. To get to a physics object from just a number
of voltages, a lot of work has to be done. This process of converting from raw hits
<span class="footnote-mark"><a 
href="main7.html#fn6x0"><sup class="textsuperscript">6</sup></a></span><a 
 id="x1-24001f6"></a> to
physics objects is called reconstruction.
<!--l. 9--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 12--><p class="noindent" ><img 
src="figures/mod0Events.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-24003r19"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;19. </span><span  
class="content">Some reconstructed events from the module-0 prototype for the DUNE Near Detector </span></div><!--tex4ht:label?: x1-24003r19 -->
                                                                                         
                                                                                         
<!--l. 15--><p class="indent" >      </div><hr class="endfigure">
<!--l. 17--><p class="indent" >      There are a number of algorithims to do reconstruction of events, oftentimes with their own
idiosyncracies, advantages and disadvantages. The Scalable Particle Imaging with Neural Embeddings
(SPINE) package is one that uses machine learning to perform this reconstruction.
<!--l. 20--><p class="indent" >      The SPINE package is designed to work with LArTPC detectors with pixellated charge readout
planes. Traditionally, LArTPC detectors have used wire planes. These generate a series of
2D pictures rather than a native 3D image that is generated by a pixellated readout plane.
<span class="footnote-mark"><a 
href="main8.html#fn7x0"><sup class="textsuperscript">7</sup></a></span><a 
 id="x1-24004f7"></a> The
modules that are going to be part of the <span 
class="zptmcm7t-x-x-120">2</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">2 </span>prototype have pixellated planes, so we are good to use
SPINE here.
<!--l. 28--><p class="indent" >      We start with a root file produced either by simulation or with real LArTPC data. That then gets run
through LArCV which is a C++ library to process LArTPC images. LArCV processes the root files into
what is called a sparse 3D input; just a list of coordinates that correspond to non-zero voxels (3d
pixels)
<!--l. 32--><p class="indent" >      After that, we can engage SPINE proper.
<!--l. 34--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 36--><p class="noindent" ><img 
src="figures/spineSchematic.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-24006r20"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;20.
</span><span  
class="content">Overview
of
the
SPINE
schematic
</span></div><!--tex4ht:label?: x1-24006r20 -->
                                                                                         
                                                                                         
<!--l. 39--><p class="indent" >      </div><hr class="endfigure">
<!--l. 41--><p class="indent" >      SPINE consists of a number of machine learning models stitched together, workingin
concert to get from raw hits to reconstructed outputs. Let&#8217;s break down each of the parts in
turn.
      <h4 class="subsectionHead"><span class="titlemark">4.1    </span> <a 
 id="x1-250004.1"></a>Semantic Segmentation and PPN</h4>
<!--l. 3--><p class="noindent" >The first thing to handle is semantic segmentation; all that means is voxel wise classification. For this
purpose, we use a very popular CNN model called UResNet. This model integrates the U-Net
architecture, originally designed for biomedical image segmentation, with residual learning principles
introduced by ResNet. By incorporating residual blocks into the U-Net framework, UResNet addresses
the vanishing gradient problem and enhances feature learning across different layers of the network. This
integration is achieved through skip connections between the encoder and decoder paths, which facilitate
the flow of high-level features and spatial information, thus improving the accuracy of segmentation in
complex image datasets.
<!--l. 9--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 12--><p class="noindent" ><img 
src="figures/uresnet.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-25001r21"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;21. </span><span  
class="content">UResNet Architecture                                                           </span></div><!--tex4ht:label?: x1-25001r21 -->
                                                                                         
                                                                                         
<!--l. 15--><p class="indent" >      </div><hr class="endfigure">
<!--l. 17--><p class="indent" >      The semantic segmentation reconstruction is supposed to take the raw voxels and classify them into
one of 5 or 6 categories based on whether ghost points are included in the dataset.
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 20--><p class="noindent" >Electromagnetic showers (e.g. electrons, photons = gammas)
      </li>
      <li class="itemize">
      <!--l. 22--><p class="noindent" >Track-like particles (e.g. muons, protons, pions)
      </li>
      <li class="itemize">
      <!--l. 24--><p class="noindent" >Delta rays (electrons knocked off from hard scattering)
      </li>
      <li class="itemize">
      <!--l. 26--><p class="noindent" >Michel electrons (coming from muon decay)
      </li>
      <li class="itemize">
      <!--l. 28--><p class="noindent" >Low energy depositions
      </li>
      <li class="itemize">
      <!--l. 30--><p class="noindent" >if enabled) Ghost points
      </li></ul>
                                                                                         
                                                                                         
<!--l. 34--><p class="indent" >      Since the <span 
class="zptmcm7t-x-x-120">2</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">2 </span>has a pixellated readout plane, we don&#8217;t have to worry about ghost points in this
case.
<!--l. 36--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 39--><p class="noindent" ><img 
src="figures/semanticEvent.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-25002r22"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;22. </span><span  
class="content">Semantic Segmentation Event display comparing monte carlo truth and reconstructed
prediction                                                                                </span></div><!--tex4ht:label?: x1-25002r22 -->
                                                                                         
                                                                                         
<!--l. 42--><p class="indent" >      </div><hr class="endfigure">
<!--l. 44--><p class="indent" >      The other part of this step is the point proposal network (PPN). This is just 3 layers attached to the
UResNet model. The job of the PPN is to pick out points of interest; namely the start of a shower, the start
of a track and the end of a track.
<!--l. 48--><p class="indent" >      The performance of this stage can be seen in figure <a 
href="#x1-25003r23">23<!--tex4ht:ref: semanticPerformance --></a>.
<!--l. 50--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 53--><p class="noindent" ><img 
src="figures/semanticPerformance.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-25003r23"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;23. </span><span  
class="content">Confusion Matrix showing performance of semantic segmentation                  </span></div><!--tex4ht:label?: x1-25003r23 -->
                                                                                         
                                                                                         
<!--l. 56--><p class="indent" >      </div><hr class="endfigure">
<!--l. 58--><p class="indent" >      Figure <a 
href="#x1-25003r23">23<!--tex4ht:ref: semanticPerformance --></a> is a confusion matrix. It has numbers that indicate how often the model is
getting the right answers. The rows are predictions while the columns are the label from the
simulation.
<!--l. 60--><p class="indent" >      The full SPINE pipeline was trained on a Set of 400k (train) + 100k (validation) 2x2 simulated
images. Isotropic particle bombs (<span 
class="zptmcm7m-x-x-120">&#x03BD;</span>-like)overlayed with isotropic particles (rock-like) So the performance
numbers reflect this training environment.
      <h4 class="subsectionHead"><span class="titlemark">4.2    </span> <a 
 id="x1-260004.2"></a>Particle Clustering</h4>
<!--l. 3--><p class="noindent" >Once the semantic segmentation is done, their results get passed onto the spice GNN. The purpose of this
GNN is to group different voxels together if they belong to the same shower or track. The SPICE
(Sparsity-preserving Invariant Convolutional Embedding) model is a novel approach designed to address
challenges in graph-based learning by leveraging sparsity and invariance principles. At its core, SPICE
utilizes convolutional operations that preserve the structural sparsity of graphs, which is crucial
for efficiently processing large-scale and complex graph data. Lucky for us, neutrino events
are incredibly sparse. By maintaining sparsity, SPICE reduces computational overhead and
memory usage, making it scalable to larger graphs. The invariant convolutional embeddings
produced by SPICE ensure that the model is robust to transformations and perturbations in graph
structures, such as node and edge additions or deletions, thereby enhancing its generalization
capabilities.
<!--l. 11--><p class="indent" >      In practical terms, SPICE integrates several advanced techniques to optimize graph learning tasks.
The model incorporates a sparse convolutional layer that operates directly on the non-zero elements of
graph adjacency matrices, bypassing the need for dense matrix operations that can be computationally
expensive. Additionally, SPICE applies invariant transformations that allow the network to remain
effective even when the graph undergoes structural changes.
<!--l. 15--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 18--><p class="noindent" ><img 
src="figures/gnn.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-26001r24"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;24. </span><span  
class="content">Architecture of the clustering GNN                                               </span></div><!--tex4ht:label?: x1-26001r24 -->
                                                                                         
                                                                                         
<!--l. 21--><p class="indent" >      </div><hr class="endfigure">
<!--l. 23--><p class="indent" >      This clustering is done in separate parts between the tracks and showers before coming together to
cluster interactions.
<!--l. 25--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 28--><p class="noindent" ><img 
src="figures/clusteringPerformance.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-26002r25"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;25. </span><span  
class="content">Box and whisker plot showing performance of clustering                           </span></div><!--tex4ht:label?: x1-26002r25 -->
                                                                                         
                                                                                         
<!--l. 31--><p class="indent" >      </div><hr class="endfigure">
<!--l. 33--><p class="indent" >      Due to the modular nature of the <span 
class="zptmcm7t-x-x-120">2</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="zptmcm7t-x-x-120">2 </span>prototype , there can be quite large gaps between 2 parts of
the same track or in the middle of a shower where the modules switch over making the job of clustering
more difficult than it would be in a monolithic detector.
      <h4 class="subsectionHead"><span class="titlemark">4.3    </span> <a 
 id="x1-270004.3"></a>Particle Identification</h4>
<!--l. 3--><p class="noindent" >Now that things are clustered properly, we move on to do particle identification like in figure
<a 
href="#x1-27001r26">26<!--tex4ht:ref: pidEvent --></a>.
<!--l. 5--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 8--><p class="noindent" ><img 
src="figures/pidEvent.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-27001r26"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;26. </span><span  
class="content">Particle ID Event display comparing monte carlo truth and reconstructed prediction   </span></div><!--tex4ht:label?: x1-27001r26 -->
                                                                                         
                                                                                         
<!--l. 11--><p class="indent" >      </div><hr class="endfigure">
<!--l. 13--><p class="indent" >      This is done through the Graph Partitioning and Aggregation (GrapPa GNN.GrapPa
innovates by combining partitioning and aggregation techniques to handle massive graphs
efficiently. It first partitions the input graph into smaller, manageable subgraphs, ensuring that
computational resources are allocated more effectively. These partitions are then processed
independently through GNN layers, which helps in capturing local graph structures without
overwhelming the system&#8217;s memory. The aggregation step combines the results from these
subgraphs, allowing the model to capture global dependencies and maintain a comprehensive
understanding of the entire graph&#8217;s topology. This approach strikes a balance between efficiency and
expressiveness, making it particularly useful for applications with large and complex graph
data.
<!--l. 19--><p class="indent" >      The GrapPa framework also integrates advanced techniques such as hierarchical pooling and
dynamic batching to further enhance its scalability and performance. Hierarchical pooling enables the
model to learn and propagate information across different levels of graph granularity, while
dynamic batching adjusts the processing load according to the current computational demands.
These features contribute to GrapPa&#8217;s ability to generalize well across various graph sizes and
structures, addressing key limitations of traditional GNNs which often struggle with scalability and
efficiency.
<!--l. 23--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 26--><p class="noindent" ><img 
src="figures/pidPerformance.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-27002r27"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;27. </span><span  
class="content">Confusion Matrix showing performance of Particle Identification                   </span></div><!--tex4ht:label?: x1-27002r27 -->
                                                                                         
                                                                                         
<!--l. 29--><p class="indent" >      </div><hr class="endfigure">
<!--l. 31--><p class="indent" >      GrapPa tries not only to identify the individual particles but also runs a binary classifier to see if the
particle is a primary or secondary.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">5    </span> <a 
 id="x1-280005"></a>Future Steps</h3>
<!--l. 3--><p class="noindent" >I believe it is highly unlikely that DUNE will gather reasonable amounts of data that a physics
analysis can be performed on in the next couple of years. So, I wish to take a look at data
that has already been gathered at NOVA. The hope is to extend the analysis that Sarah
Choate is working on that tries to detect the anomolous magnetic moment of a neutrino.
<span class="footnote-mark"><a 
href="main9.html#fn8x0"><sup class="textsuperscript">8</sup></a></span><a 
 id="x1-28001f8"></a> It
entails looking for a low energy final state electron. Currently there is a CNN that has been trained to
identify electrons, but that was trained on standard model data and has difficulty distinguishing between
Michel electrons and the final state electrons that result from the hypothetical coupling of the muon
neutrino to a photon.
<!--l. 10--><p class="indent" >      I believe the work done on DUNE on machine learning based reconstruction leaves me
in a good position to work on the problem of improving reconstruction at the low energy
regime for NOVA despite it being a scintillator based detector rather than a LArTPC. With
improved reconstruction, the analysis performed in the <span 
class="zptmcm7t-x-x-120">0</span><span 
class="zptmcm7m-x-x-120">.</span><span 
class="zptmcm7t-x-x-120">5</span><span 
class="zptmcm7m-x-x-120">GeV </span>should lead to a more precise
result.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-29000"></a>References</h3>
<a 
 id="Q1-1-60"></a>
<!--l. 1--><p class="noindent" >
     <div class="thebibliography">
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XOerter"></a>[1] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>R.&#x00A0;Oerter.  <span 
class="ptmri7t-x-x-120">The Theory of Almost Everything: The Standard Model, the Unsung Triumph</span>
     <span 
class="ptmri7t-x-x-120">of Modern Physics</span>. Penguin Publishing Group, 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XTimaeus"></a>[2] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Plato John&#x00A0;Warrington.  <span 
class="ptmri7t-x-x-120">Tmaeus - Plato ; edited and translated with an introduction by</span>
     <span 
class="ptmri7t-x-x-120">John Warrington</span>. Dent ; Dutton, 1965.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XDalton"></a>[3] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Harold  Hartley.    John  dalton,  f.r.s.  (1766-1844)  and  the  atomic  theory-a  lecture  to
     commemorate  his  bicentenary.    <span 
class="ptmri7t-x-x-120">Proceedings  of  the  Royal  Society  of  London.  Series  B,</span>
     <span 
class="ptmri7t-x-x-120">Biological Sciences</span>, 168(1013):335&#8211;359, 1967.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XelectronDiscovery"></a>[4] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>J.&#x00A0;J. Thomson. The electron. <span 
class="ptmri7t-x-x-120">The Scientific Monthly</span>, 20(2):113&#8211;115, 1925.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XPullman"></a>[5] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>B.&#x00A0;Pullman. <span 
class="ptmri7t-x-x-120">The Atom in the History of Human Thought</span>. Oxford University Press, 1998.
     </p>
                                                                                         
                                                                                         
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XThomson"></a>[6] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>J.&#x00A0;J.  Thomson.    Cathode  rays.    <span 
class="ptmri7t-x-x-120">The  London,  Edinburgh,  and  Dublin  Philosophical</span>
     <span 
class="ptmri7t-x-x-120">Magazine and Journal of Science</span>, 44(269):293&#8211;316, Oct 1897.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XThorpe"></a>[7] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>T.&#x00A0;E. THORPE.  On the relation between the molecular weights of substances and their
     specific gravities when in the liquid state. <span 
class="ptmri7t-x-x-120">Nature</span>, 22(560):262&#8211;263, Jul 1880.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XThomson_1907"></a>[8] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>J.&#x00A0;J. Thomson. <span 
class="ptmri7t-x-x-120">The corpuscular theory of matter by J.J. Thomson . </span>A. Constable, 1907.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XPPmodel"></a>[9] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Dec 2005.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XNagaoka"></a>[10] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hantaro Nagaoka.  The inductance coefficients of solenoids.  <span 
class="ptmri7t-x-x-120">Journal of the College of</span>
     <span 
class="ptmri7t-x-x-120">Science</span>, 1909.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XTibbetts"></a>[11] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Gary&#x00A0;G.  Tibbetts.   <span 
class="ptmri7t-x-x-120">How  the  great  scientists  reasoned  the  scientific  method  in  action</span>.
     Elsevier Science &amp; Technology Books, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XBelyaev_Ross"></a>[12] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alexander  Belyaev  and  Douglas  Ross.    <span 
class="ptmri7t-x-x-120">The  basics  of  nuclear  and  particle  physics</span>.
     Springer, 2021.
                                                                                         
                                                                                         
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XAtomic_Nucleus"></a>[13] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael&#x00A0;F. L&#8217;Annunziata. Chapter 20 - the atomic nucleus. In Michael&#x00A0;F. L&#8217;Annunziata,
     editor,  <span 
class="ptmri7t-x-x-120">Radioactivity  (Second  Edition)</span>,  pages  679&#8211;728.  Elsevier,  Boston,  second  edition
     edition, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XGM_Experiment"></a>[14] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Wikimedia  Commons.   File:geiger-marsden  experiment  expectation  and  result.svg  &#8212;
     wikimedia commons, the free media repository, 2024.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XBaily"></a>[15] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C.&#x00A0;Baily.    Early  atomic  models  &#8211;  from  mechanical  to  quantum  (1904&#8211;1913)  -  the
     european physical journal h, Oct 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XRutherford_1911"></a>[16] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>E.&#x00A0;Rutherford.   Lxxix. the scattering of <span 
class="zptmcm7m-x-x-120">&#x03B1; </span>and <span 
class="zptmcm7m-x-x-120">&#x03B2; </span>particles by matter and the structure
     of the atom.  <span 
class="ptmri7t-x-x-120">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of</span>
     <span 
class="ptmri7t-x-x-120">Science</span>, 21(125):669&#8211;688, May 1911.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XRutherford_model"></a>[17] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Sep 2024.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XKopot"></a>[18] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrey Kopot. Flaws in rutherford&#8217;s model of the atom, Feb 2014.
     </p>
                                                                                         
                                                                                         
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XBohr_1913"></a>[19] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>N.&#x00A0;Bohr.  I. on the constitution of atoms and molecules.  <span 
class="ptmri7t-x-x-120">The London, Edinburgh, and</span>
     <span 
class="ptmri7t-x-x-120">Dublin Philosophical Magazine and Journal of Science</span>, 26(151):1&#8211;25, Jul 1913.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XNewton_1948"></a>[20] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Isaac  Newton.    A  new  theory  of  light  and  colors,  1672.    <span 
class="ptmri7t-x-x-120">Readings  in  the  history  of</span>
     <span 
class="ptmri7t-x-x-120">psychology.</span>, page 44&#8211;54, 1948.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XBohr_model"></a>[21] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Sep 2024.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XHooke_1667"></a>[22] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Robert Hooke. <span 
class="ptmri7t-x-x-120">Micrographia: Or, some physiological descriptions of minute bodies made</span>
     <span 
class="ptmri7t-x-x-120">by magnifying glasses. with observations and inquiries thereupon</span>.  Printed for John Martyn,
     printer to the Royal Society, 1667.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XHuygens_1690"></a>[23] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C.&#x00A0;Huygens.   <span 
class="ptmri7t-x-x-120">Traite de la lumiere. O</span><span 
class="ptmri7t-x-x-120">ù sont expliqu</span><span 
class="ptmri7t-x-x-120">ées les causes de ce qui luy arrive</span>
     <span 
class="ptmri7t-x-x-120">dans la reflexion, &amp; dans la refraction. Et particulierment dans l&#8217;etrange refraction du cristal</span>
     <span 
class="ptmri7t-x-x-120">d&#8217;Islande, par C.H.D.Z. Avec un Discours de la cause de la pesanteur</span>.  chez Pierre Vander
     Aa marchand libraire, 1690.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XYoung_1804"></a>[24] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Thomas Young.  The bakerian lecture. experiments and calculations relative to physical
     optics. <span 
class="ptmri7t-x-x-120">Philosophical Transactions of the Royal Society of London</span>, 94:1&#8211;16, Dec 1804.
                                                                                         
                                                                                         
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XDS_experiment"></a>[25] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Aug 2024.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XPlanck_1901"></a>[26] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Max Planck.  Ueber das gesetz der energieverteilung im normalspectrum.  <span 
class="ptmri7t-x-x-120">Annalen der</span>
     <span 
class="ptmri7t-x-x-120">Physik</span>, 309(3):553&#8211;563, Jan 1901.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XEinstein_1905"></a>[27] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A.&#x00A0;Einstein.     Über  einen  die  erzeugung  und  verwandlung  des  lichtes  betreffenden
     heuristischen gesichtspunkt. <span 
class="ptmri7t-x-x-120">Annalen der Physik</span>, 322(6):132&#8211;148, Jan 1905.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XEinstein_1905a"></a>[28] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A.&#x00A0;Einstein.         Zur   elektrodynamik   bewegter   körper.         <span 
class="ptmri7t-x-x-120">Annalen   der   Physik</span>,
     322(10):891&#8211;921, Jan 1905.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XLEWIS_1926"></a>[29] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>GILBERT&#x00A0;N. LEWIS.  The conservation of photons.  <span 
class="ptmri7t-x-x-120">Nature</span>, 118(2981):874&#8211;875, Dec
     1926.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XMillikan_1914"></a>[30] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>R.&#x00A0;A. Millikan. A direct determination of h. <span 
class="ptmri7t-x-x-120">Physical Review</span>, 4(1):73&#8211;75, Jul 1914.
     </p>
                                                                                         
                                                                                         
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XHeisenberg_1927"></a>[31] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>W.&#x00A0;Heisenberg.   Ber den anschaulichen inhalt der quantentheoretischen kinematik und
     mechanik. <span 
class="ptmri7t-x-x-120">Zeitschrift fur Physik</span>, 43(3&#8211;4):172&#8211;198, Mar 1927.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XHeisenberg_1971"></a>[32] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>W.&#x00A0;Heisenberg.      <span 
class="ptmri7t-x-x-120">Physics   and   Beyond:   Encounters   and   Conversations</span>.      Harper
     Torchbooks. Harper &amp; Row, 1971.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XDeBroglie_1925"></a>[33] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Louis  De&#x00A0;Broglie.     Recherches  sur  la  théorie  des  quanta.     <span 
class="ptmri7t-x-x-120">Annales  de  Physique</span>,
     10(3):22&#8211;128, 1925.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XDavisson_Germer_1927"></a>[34] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C.&#x00A0;Davisson and L.&#x00A0;H. Germer.  The scattering of electrons by a single crystal of nickel.
     <span 
class="ptmri7t-x-x-120">Nature</span>, 119(2998):558&#8211;560, Apr 1927.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XDavisson_Germer_1927a"></a>[35] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C.&#x00A0;Davisson and L.&#x00A0;H. Germer.  Diffraction of electrons by a crystal of nickel.  <span 
class="ptmri7t-x-x-120">Physical</span>
     <span 
class="ptmri7t-x-x-120">Review</span>, 30(6):705&#8211;740, Dec 1927.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XDavisson_Germer_1928"></a>[36] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C.&#x00A0;J.  Davisson  and  L.&#x00A0;H.  Germer.     Reflection  of  electrons  by  a  crystal  of  nickel.
     <span 
class="ptmri7t-x-x-120">Proceedings of the National Academy of Sciences</span>, 14(4):317&#8211;322, Apr 1928.
     </p>
                                                                                         
                                                                                         
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XSchrÃ¶dinger_1926"></a>[37] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>E.&#x00A0;Schrödinger. An undulatory theory of the mechanics of atoms and molecules. <span 
class="ptmri7t-x-x-120">Physical</span>
     <span 
class="ptmri7t-x-x-120">Review</span>, 28(6):1049&#8211;1070, Dec 1926.
</p>
     </div>
       
</body></html> 

                                                                                         


