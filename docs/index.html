<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>MACHINE LEARNING BASED RECONSTRUCTION STUDIES FOR DUNE NEAR
DETECTOR</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="main.tex"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
><div class="maketitle">
                                                                                         
                                                                                         
                                                                                         
                                                                                         

<h2 class="titleHead">MACHINE LEARNING BASED RECONSTRUCTION STUDIES FOR DUNE NEAR
DETECTOR</h2>
<div class="author" ></div><br />
<div class="date" ></div>
      </div>
<!--l. 218--><p class="indent" >      &#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />
<div class="center" 
>
<!--l. 219--><p class="noindent" >
<!--l. 220--><p class="noindent" >by</div>
<div class="center" 
>
<!--l. 223--><p class="noindent" >
<!--l. 224--><p class="noindent" >Orgho Neogi</div>
<!--l. 226--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />&#x00A0;<br 
class="newline" />
<div class="center" 
>
<!--l. 227--><p class="noindent" >
<!--l. 228--><p class="noindent" >A thesis submitted in partial fulfillment<br />
of the requirements for the Doctor of Philosophy<br />
degree in Physics &amp; Astronomy&#x00A0;in the<br />
Graduate College of<br />
The University of Iowa</div>
<div class="center" 
>
<!--l. 235--><p class="noindent" >
                                                                                         
                                                                                         
<!--l. 236--><p class="noindent" >May&#x00A0;2024</div>
<div class="center" 
>
<!--l. 239--><p class="noindent" >
<!--l. 240--><p class="noindent" >Thesis Committee: Name of Thesis Supervisor, Jane Nachtman</div>
<div class="center" 
>
<!--l. 242--><p class="noindent" >
<!--l. 243--><p class="noindent" >Yaser Onel</div>
<div class="center" 
>
<!--l. 245--><p class="noindent" >
<!--l. 246--><p class="noindent" >Milind Diwan</div>
<div class="center" 
>
<!--l. 248--><p class="noindent" >
<!--l. 249--><p class="noindent" >Mary Hall Reno</div>
                                                                                         
                                                                                         
<blockquote class="quote">
<!--l. 4--><p class="noindent" >I have yet to see any problem, however complicated, which, when looked at in the right way did
not become still more complicated.
<!--l. 7--><p class="noindent" >Poul Anderson<br />
</blockquote>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<div class="center" 
>
<!--l. 2--><p class="noindent" >
<!--l. 3--><p class="noindent" >ABSTRACT </div>
<!--l. 7--><p class="indent" >      Prior to your first thesis deposit, replace this text with the text of your scientific/ scholarly
abstract. The text of this abstract should be double spaced and each new paragraph should be
indented.
<div class="center" 
>
<!--l. 11--><p class="noindent" >
<!--l. 12--><p class="noindent" ><span 
class="ptmb7t-x-x-120">This abstract is required for everyone except DMA and MFA students.</span></div>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<div class="center" 
>
<!--l. 2--><p class="noindent" >
<!--l. 3--><p class="noindent" >PUBLIC ABSTRACT </div>
<!--l. 7--><p class="indent" >      Prior to your thesis deposit, replace this text with the text of your public abstract. The text of this
abstract should be double spaced and each new paragraph should be indented.
<!--l. 9--><p class="indent" >      <span 
class="ptmb7t-x-x-120">This abstract is required for all thesis/dissertations. </span>This abstract may be up to 250 words and
should be written for a non-academic lay audience. In writing your public abstract, avoid jargon and
technical language as much as possible.
<!--l. 11--><p class="indent" >      The ability to communicate research simply and clearly is an important skill. The public abstract
helps convey ideas beyond one&#8217;s immediate academic circle, facilitating communication with colleagues
who do different kinds of work and possess different dimensions of training.
<!--l. 13--><p class="indent" >      Think of your public abstract as your &#8220;elevator pitch&#8221; or what you might tell someone who asks,
&#8220;What is your thesis about?&#8221; You may only have a few minutes to explain it to them while keeping their
attention and using terminology you are sure they will understand without further lengthy
explanation.
<!--l. 15--><p class="indent" >      Another way to think of your public abstract is like the description you would read on the inside of
a book cover.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-1000"></a>table of contents</h3>
      <div class="tableofcontents">
      &#x00A0;<span class="sectionToc" ><a 
href="#Q1-1-3">List of Tables</a></span>
<br />      &#x00A0;<span class="sectionToc" ><a 
href="#Q1-1-5">List of Figures</a></span>
<br />      &#x00A0;<span class="sectionToc" >1 <a 
href="#x1-40001" id="QQ2-1-6">Introduction</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.1 <a 
href="#x1-50001.1" id="QQ2-1-7">The Standard Model</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.2 <a 
href="#x1-60001.2" id="QQ2-1-8">The Electron</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.3 <a 
href="#x1-70001.3" id="QQ2-1-9">Dalton&#8217;s Atomic Theory</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.4 <a 
href="#x1-80001.4" id="QQ2-1-10">The Plum Pudding Model</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.5 <a 
href="#x1-90001.5" id="QQ2-1-12">Gold Foil Experiments</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.6 <a 
href="#x1-100001.6" id="QQ2-1-14">Rutherford&#8217;s Model</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.7 <a 
href="#x1-110001.7" id="QQ2-1-16">Bohr&#8217;s Model</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.8 <a 
href="#x1-120001.8" id="QQ2-1-18">The Photon</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >1.9 <a 
href="#x1-130001.9" id="QQ2-1-20">Electroweak Interactions</a></span>
<br />      &#x00A0;<span class="sectionToc" >2 <a 
href="#x1-140002" id="QQ2-1-21">Neutrino Detection</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >2.1 <a 
href="#x1-150002.1" id="QQ2-1-22">Detector Types</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >2.2 <a 
href="#x1-160002.2" id="QQ2-1-23">DUNE</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >2.3 <a 
href="#x1-170002.3" id="QQ2-1-24">NOVA</a></span>
<br />      &#x00A0;<span class="sectionToc" >3 <a 
href="#x1-180003" id="QQ2-1-25">Machine Learning</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.1 <a 
href="#x1-190003.1" id="QQ2-1-28">Perceptron Neuron</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.2 <a 
href="#x1-200003.2" id="QQ2-1-31">Sigmoid Neuron</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.3 <a 
href="#x1-210003.3" id="QQ2-1-33">Activation Functions</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.4 <a 
href="#x1-220003.4" id="QQ2-1-34">Neural Network</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.5 <a 
href="#x1-230003.5" id="QQ2-1-36">Gradient Descent</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.6 <a 
href="#x1-240003.6" id="QQ2-1-37">Convolutional Neural Network</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.7 <a 
href="#x1-250003.7" id="QQ2-1-38">Graph Neural Network</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.8 <a 
href="#x1-260003.8" id="QQ2-1-39">Model Development</a></span>
<br />      &#x00A0;&#x00A0;<span class="subsectionToc" >3.9 <a 
href="#x1-270003.9" id="QQ2-1-41">Model Optimization</a></span>
<br />      &#x00A0;<span class="sectionToc" ><a 
href="#Q1-1-43">References</a></span>
      </div>
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-2000"></a>List of Tables</h3>
<a 
 id="Q1-1-3"></a>
      <div class="tableofcontents">
      </div>
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-3000"></a>List of Figures</h3>
<a 
 id="Q1-1-5"></a>
      <div class="tableofcontents"><span class="lofToc" >1&#x00A0;<a 
href="#x1-8003r1">Cartoon  of  plum  pudding  model</a></span><br /><span class="lofToc" >2&#x00A0;<a 
href="#x1-9001r2">Cartoon  of  Gold  foil
experiment</a></span><br /><span class="lofToc" >3&#x00A0;<a 
href="#x1-10001r3">Rutherford&#8217;s atomic model</a></span><br /><span class="lofToc" >4&#x00A0;<a 
href="#x1-11001r4">Bohr&#8217;s model of the hydrogen
atom</a></span><br /><span class="lofToc" >5&#x00A0;<a 
href="#x1-12001r5">Double Slit Experiment</a></span><br /><span class="lofToc" >6&#x00A0;<a 
href="#x1-18001r6">Example of a basic decision tree</a></span><br /><span class="lofToc" >7&#x00A0;<a 
href="#x1-18002r7">Example
of a basic Neural Net</a></span><br /><span class="lofToc" >8&#x00A0;<a 
href="#x1-19003r8">Perceptron Neuron</a></span><br /><span class="lofToc" >9&#x00A0;<a 
href="#x1-19005r9">Perceptron network</a></span><br /><span class="lofToc" >10&#x00A0;<a 
href="#x1-20004r10">Sigmoid
Function</a></span><br /><span class="lofToc" >11&#x00A0;<a 
href="#x1-22001r11">Parts of a Neural Net</a></span><br /><span class="lofToc" >12&#x00A0;<a 
href="#x1-26001r12">Model development</a></span><br />
      </div>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-40001"></a>Introduction</h3>
<!--l. 3--><p class="noindent" >I have yet to see any problem, however complicated, which, when looked at in the right way did not
become still more complicated.
                                                                                         <div class="flushright" 
>
<!--l. 5--><p class="noindent" >
&#8212; Poul Anderson</div>
<!--l. 7--><p class="indent" >      People working in the field of high energy physics have a tendency to concern themselves with
attempting to solve problems that are incredibly complicated. So, perhaps, there is a touch
of irony that the problem that they are trying to solve is not only incredibly fundamental,
but also very simple to state. The question can be boiled down to &#8211; what is the stuff in our
universe made of? What immediately follows from this fundamental inquiry is how is matter
made up of these things ; or to put it another way, how do the fundamental building blocks
interact.
<!--l. 12--><p class="indent" >      In some sense particle physics tries to distill matter and the interactions therein down to the
smallest possible level to which it can be broken down. Turns out that breaking these concepts down to
this elementary level of specificity is an incredibly complicated process of which we have merely begun to
scratch the surface. As such,this paper focuses on a tiny fraction of these fundamental building blocks &#8211;
the elusive neutrino with the hope of just perhaps being able to untangle some of the myriad of secrets that
it harbours.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">1.1    </span> <a 
 id="x1-50001.1"></a>The Standard Model</h4>
                                                                                         
                                                                                         
<!--l. 3--><p class="noindent" >Before the protagonist <span class="footnote-mark"><a 
href="main2.html#fn1x0"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-5001f1"></a> 
of our story - the neutrino - can be formally introduced, the stage has to be set. A good candidate to set the
stage would be the standard model which describesthree of the four known fundamental forces,
electromagnetic, weak and strong interactions (it struggles to deal with gravity) and classifying all known
elementary particles. Just like any foundational theory that undergirds a subfield of a subject, the standard
model definitely wasn&#8217;t developed in a day and as such, it may behoove us to at least go over the high
points ofits development in order to have a better understanding of the context that surrounds
neutrinos.
<!--l. 9--><p class="indent" >      One may definitely quibble about where our understanding of the fundamental particles starts from,
after all, humans have been trying to find out the nature of our universe and the things that make it up
going back as far as the 4th century BCE with Plato positing that everything is made up of 4
elements (water, wind, earth and fire)[<a 
href="#XTimaeus">1</a>] but I think it makes sense to start at the discovery
of the first of the particles that made it&#8217;s way into the pantheon of the standard model; the
electron.
      <h4 class="subsectionHead"><span class="titlemark">1.2    </span> <a 
 id="x1-60001.2"></a>The Electron</h4>
<!--l. 3--><p class="noindent" >For the longest time, humans had thought that atoms were the smallest particle that makes up
everything in the world and cannot be subdivided further[<a 
href="#XDalton">2</a>] but this idea had started to come
under scrutiny by the late 1800&#8217;s. Even then, it was thought that if anything were to make up
atoms, they would n&#8217;t be lighter than the lightest atom. However, in 1897, Thomson would
come in with evidence that there not only were particles that made up the atoms, but that
they were on the scale of 1000 times lighter than hydrogen. He decided to shoot cathode rays
at a thermal junction so he could measure the generated heat and neasured how much they
                                                                                         
                                                                                         
deflectedmagnetically. He also measured the electrical deflections by lowering the pressure in the chamber
where he was measuring the deflection. Through these experiments, discovered the electron
<span class="footnote-mark"><a 
href="main3.html#fn2x0"><sup class="textsuperscript">2</sup></a></span><a 
 id="x1-6001f2"></a> and
believed that it was a fundamental part of all atoms that was very light and held a decidedly negative
charge.[<a 
href="#XelectronDiscovery">3</a>]
      <h4 class="subsectionHead"><span class="titlemark">1.3    </span> <a 
 id="x1-70001.3"></a>Dalton&#8217;s Atomic Theory</h4>
<!--l. 3--><p class="noindent" >The discovery of something so much smaller than the lightest atom threw Dalton&#8217;s atomic theory out the
window. His theory claimed that everything in the universe was made up of atoms which would vary in
size and mass based on the element. These atoms could not be created or destroyed, but could
reaarrange themselves through chemical reactions. It could be argued that Dalton&#8217;s model was a
progenitor for the idea of conservation of mass and energy. Despite being such an important idea,
even before the discovery of electrons, the theory wasn&#8217;t fullproof; it could not account for
isotopes of the same element having different masses, but the electron blew the idea wide
apart.
<!--l. 9--><p class="indent" >      A new theory that looked at the atom not as the smallest thing that could exist but rather something
that had other things inside in some sort of structure had to be developed.
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">1.4    </span> <a 
 id="x1-80001.4"></a>The Plum Pudding Model</h4>
<!--l. 3--><p class="noindent" >There were numerous models that tried to tackle this problem and one of the first was proposed by
Thompson in 1904 as the plum pudding model. The first problem to grapple with was that electrons are
negatively charged while the atoms themselves are electrically neutral. To get around this, the plum
                                                                                         
                                                                                         
pudding model suggests that the electrons were suspended in a morass of positively charged particles
<span class="footnote-mark"><a 
href="main4.html#fn3x0"><sup class="textsuperscript">3</sup></a></span><a 
 id="x1-8001f3"></a> with
the charge between the positive and negative equalling out to 0. Thomson believed that the mass was
evenly distributed throughout the atom.
<!--l. 10--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 13--><p class="noindent" ><img 
src="figures/plumPudding.png" alt="PIC"  
width="284" height="284" > <a 
 id="x1-8003r1"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;1. </span><span  
class="content">Cartoon of plum pudding model                                                   </span></div><!--tex4ht:label?: x1-8003r1 -->
                                                                                         
                                                                                         
<!--l. 16--><p class="indent" >      </div><hr class="endfigure">
<!--l. 18--><p class="indent" >      The plum pudding model struggled to explain how these charged particles were so copacetic with
each other despite being such small physical distances apart. It was well known by then that opposite
charges attract while alike charges repel. It also failed to provide any explanation of the spectral lines
observed in hydrogen. Darker clouds were still on the horizon for Thomson&#8217;s plum pudding
model.
      <h4 class="subsectionHead"><span class="titlemark">1.5    </span> <a 
 id="x1-90001.5"></a>Gold Foil Experiments</h4>
<!--l. 3--><p class="noindent" >Between 1908 and 1913, a number of alpha (<span 
class="zptmcm7m-x-x-120">&#x03B1;</span>) particle scattering experiments were performed by Hans
Geiger and Ernest Marsden. These took the form of shooting <span 
class="zptmcm7m-x-x-120">&#x03B1; </span>particles at a incredibly thin piece of gold
foil. Based on the plum pudding model, it was expected that the <span 
class="zptmcm7m-x-x-120">&#x03B1; </span>particles would not be deflected
however this turned out not to be the case at all. To be fair, most of the <span 
class="zptmcm7m-x-x-120">&#x03B1; </span>particles did indeed go straight
through the gold foil, their trajectory not disturbed in the slightest. A smaller fraction did get deflected,
some by a small angle and others by a large one. But the astonishing part was that an even
smaller fraction, about 1 in 20000, shot right back at the direction the particle gun was shooting
from.
<!--l. 10--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 13--><p class="noindent" ><img 
src="figures/goldFoil.png" alt="PIC"  
width="284" height="284" > <a 
 id="x1-9001r2"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;2. </span><span  
class="content">Cartoon of Gold foil experiment                                                   </span></div><!--tex4ht:label?: x1-9001r2 -->
                                                                                         
                                                                                         
<!--l. 16--><p class="indent" >      </div><hr class="endfigure">
      <h4 class="subsectionHead"><span class="titlemark">1.6    </span> <a 
 id="x1-100001.6"></a>Rutherford&#8217;s Model</h4>
<!--l. 3--><p class="noindent" >So a new model was required to explain the discrepencies away; in comes Rutherford. He looked at the
gold foil experiments done before him and ran with it, expanding upon them and developing a new theory
on the substructure of the atom. He proposed in 1911 that atoms were mostly just empty spacewith a
highly concentrated segment of mass at the center of the atom &#8211; he called this central mass the neucleus of
the atom. In Rutherford&#8217;s atomic model, the electrons orbit around the positively charged
neucleus.
<!--l. 8--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 11--><p class="noindent" ><img 
src="figures/rutherford.png" alt="PIC"  
width="284" height="284" > <a 
 id="x1-10001r3"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;3. </span><span  
class="content">Rutherford&#8217;s atomic model                                                       </span></div><!--tex4ht:label?: x1-10001r3 -->
                                                                                         
                                                                                         
<!--l. 14--><p class="indent" >      </div><hr class="endfigure">
<!--l. 16--><p class="indent" >      Only, one little problem. When things move in a circular orbit, they are accelerating and when a
charged particle is moving in an orbit like that, it should be constantly radiating energy leading to it
eventually falling into the neucleus rendering this formulation of the atom unstable. It should also be
emitting a continous energy spectrum from the electrons, but hydrogen has discrete spectral
lines.
      <h4 class="subsectionHead"><span class="titlemark">1.7    </span> <a 
 id="x1-110001.7"></a>Bohr&#8217;s Model</h4>
<!--l. 3--><p class="noindent" >Bohr tried to come at this from an angle that resolved the spectral line issue with Rutherford&#8217;s model.
Bohr proposed that electrons move in fixed orbits, thus explaining the discrete lines of the hydrogen
spectra and that atoms emit light when an electron jumps from a higher energy level to a lower
one.
<!--l. 6--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 9--><p class="noindent" ><img 
src="figures/bohr.png" alt="PIC"  
width="284" height="284" > <a 
 id="x1-11001r4"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;4. </span><span  
class="content">Bohr&#8217;s model of the hydrogen atom                                                </span></div><!--tex4ht:label?: x1-11001r4 -->
                                                                                         
                                                                                         
<!--l. 12--><p class="indent" >      </div><hr class="endfigure">
<!--l. 14--><p class="indent" >      This still doesn&#8217;t explain away why the electron doesn&#8217;t collapse into the neucleus . However, it
does a very good job of modelling hydrogen and hydrogen-like atoms under most normal conditions.
The other issue with Bohr&#8217;s model is that it fails to adress De-Broglie&#8217;s Hypothesis of the
dual nature of matter.. To get there, we have to delve into the wonderful world of quantum
mechanics.
      <h4 class="subsectionHead"><span class="titlemark">1.8    </span> <a 
 id="x1-120001.8"></a>The Photon</h4>
<!--l. 3--><p class="noindent" >What led to the development of quantum mechanics was spirited debate about the true nature of light.
Newton was one of the first to throw his hat into the ring; in 1672 he decided to build upon the corpuscular
theory coined by Descartes, arguing that light was made up of discrete particles just like everything else.
Problem was, that around the same time Robert Hooke and Christian Huygens performed experiments that
led them to believe light was in fact not a stream of particles but rather a wave. This wave
view of light did a much better job of explaining how light refracted compared to Newtons
model.
<!--l. 8--><p class="indent" >      The position of people beliving that light was in fact a wave, not particles got a lot stronger in 1801
thanks to double slit experiments by Thomas Young. This was an experiment where there were two slits
cut into a screen and light was then shone through it being visible on another screen once it had made it
past the slits. If light was indeed made up of particles, the expectation was that we would see essentially 2
bright spots on the final screenthat corresponded to the two slits. Instead, what we got was an interference
pattern that iss typical of waves.
<!--l. 13--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 16--><p class="noindent" ><img 
src="figures/doubleSlit.png" alt="PIC"  
width="284" height="284" > <a 
 id="x1-12001r5"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;5. </span><span  
class="content">Double Slit Experiment                                                          </span></div><!--tex4ht:label?: x1-12001r5 -->
                                                                                         
                                                                                         
<!--l. 19--><p class="indent" >      </div><hr class="endfigure">
<!--l. 21--><p class="indent" >      At this point, the world is pretty much in the wave camp for the purposes of modelling light, but the
idea that light is made of particles was about to be revived from the dead by none other than Max Planck.
He was trying to solve the problem of black body radiation; namely, that the energy carried by
electromagnetic waves is emitted and absorbed in discrete quantities. His solution was to come up with
the idea of discrete quanta rather than a continously emmisive spectrum. He did this through the creation
of the what we call the Planck constant today (<span 
class="zptmcm7m-x-x-120">h</span>), a proportionality constant that he called the quantum of
action.
<!--l. 26--><p class="indent" >      This was fundamentally the introduction of quantum mechanics. a quite contentious idea at the
time, highlighted by the following quote from Bohr.
<!--l. 28--><p class="indent" >      quantum theory cannot possibly have understood it.
                                                                                         <div class="flushright" 
>
<!--l. 30--><p class="noindent" >
&#8212; Neils Bohr</div>
<!--l. 32--><p class="indent" >      Despite being frought in debate, the idea of quantum mechanics simply would not go away.
Einstein would go on to build on the Planck&#8217;s ideas and proposed that light was made up of discrete
packets of energy that he called photons. He developed the Planck-Einstein Relationship, connecting
energy and the frequency of light.
<!--l. 36--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                         <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">E </span><span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7m-x-x-120">h&#x03BD;</span></td>                                         <td 
class="align-even"></td>                                         <td 
class="align-label"><a 
 id="x1-12002r1"></a>(1)                                         </td></tr></table>
<!--l. 40--><p class="indent" >      Where <span 
class="zptmcm7m-x-x-120">E </span>stands for energy, <span 
class="zptmcm7m-x-x-120">h </span>for thePlanck constant and <span 
class="zptmcm7m-x-x-120">&#x03BD; </span>for the frequency of the
photon.
<!--l. 44--><p class="indent" >      This equation was to explain the results that Einstein had gotten from his experiments
regarding the photoelectric effect. In 1914, Robert A. Millikan went on to confirm Einstein&#8217;s idea
by doing a highly accurate measurement of Plank&#8217;s Constant using the photoelectric effect.
Photons would go on to be included in the list of particles we deal with in the standard model
today.
<!--l. 48--><p class="indent" >      This new evidence went in the face of the wave nature of light which had been longstanding. It
seemed like there were phenomena that could be explained by thinking of light as a wave and other
phenomena that could be understood if we looked at light as a particle.
      <h4 class="subsectionHead"><span class="titlemark">1.9    </span> <a 
 id="x1-130001.9"></a>Electroweak Interactions</h4>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-140002"></a>Neutrino Detection</h3>
<!--l. 3--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">2.1    </span> <a 
 id="x1-150002.1"></a>Detector Types</h4>
<!--l. 5--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">2.2    </span> <a 
 id="x1-160002.2"></a>DUNE</h4>
<!--l. 7--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">2.3    </span> <a 
 id="x1-170002.3"></a>NOVA</h4>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-180003"></a>Machine Learning</h3>
<!--l. 3--><p class="noindent" >When looking at artificial intelligence (AI), everything falls on a spectrum from easily explainable to
being a black box when thinking about how the machine makes it&#8217;s decisions. On the easily explainable
side of things, we have things like decision trees.
<!--l. 6--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 8--><p class="noindent" ><img 
src="figures/decisionTree.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-18001r6"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;6. </span><span  
class="content">Example of a basic decision tree                                                  </span></div><!--tex4ht:label?: x1-18001r6 -->
                                                                                         
                                                                                         
<!--l. 11--><p class="indent" >      </div><hr class="endfigure">
<!--l. 13--><p class="indent" >      A decision tree is where we sort the data by asking a sequence of questions and following the
flowchart down to where it leads. By the time we are at the bottom of the tree and have classified the data
we can say exactly how the model does it&#8217;s classification. For instance if a decision tree is used for
mortgage decisions and the model says no, we can query and learn that it said no because you had too low
income or too low credit score for instance.
<!--l. 17--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 19--><p class="noindent" ><img 
src="figures/neuralNet1.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-18002r7"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;7. </span><span  
class="content">Example of a basic Neural Net                                                    </span></div><!--tex4ht:label?: x1-18002r7 -->
                                                                                         
                                                                                         
<!--l. 22--><p class="indent" >      </div><hr class="endfigure">
<!--l. 24--><p class="indent" >      By contrast, a machine learning model like a neural net is almost a black box with regards to how
the decisions are made. We can query the model and ask it what it made its decisions based on, however,
the features it picks out often isn&#8217;t decipherable to humans in any way. As in the previous example, if the
answer to a mortgage is no, we have no real idea why the model made that decision. That
being said, neural networks are often able to come up with better outcomes for classification
that simple models like decision trees are. In the mortgage example, even if the neural net
can&#8217;t tell us how it comes to the conclusion of approving a loan, it is still more likely to be
able to better tell who will be a good credit risk compared to the decision tree. That&#8217;s often
the trade off that we make when deciding on a more opaque model. That&#8217;s why even though
they are opaque in how they come up with their answers we still rely on them so heavily.
Because we can empirically test through monte-carlo studies how well they perform both in term
of efficiency as well as how often these models misidentify the data that we are throwing at
it.
<!--l. 33--><p class="indent" >      While a neural network is opaque about how the decisions are made, the model itself doesn&#8217;t have
to be a black box for us. We can take a peek under the hood and see how these models work. To do so, we
start up from the basic models like a perceptron and work our way to a graph neural network, finally
connecting it to how neutrino reconstruction works.
      <h4 class="subsectionHead"><span class="titlemark">3.1    </span> <a 
 id="x1-190003.1"></a>Perceptron Neuron</h4>
<!--l. 3--><p class="noindent" >A lot of things that seem incredibly easy to humans &#8211; such as recognizing the difference between say a cat
and a dog &#8211; are very difficult for computers to do. What makes it difficult to make that sort of
classification is that it is hard for humans to define concrete rules about what makes the picture of a
cat different than the picture of a dog. Neural nets approach this in a completely different
fashion.
<!--l. 7--><p class="indent" >      Instead of trying to define rules about the features that differentiate the
                                                                                         
                                                                                         
picture of a dog vs a cat, we instead classify a whole bunch of pictures by hand.
<span class="footnote-mark"><a 
href="main5.html#fn4x0"><sup class="textsuperscript">4</sup></a></span><a 
 id="x1-19001f4"></a> Then
throw those pictures at the algorithm with the correct answers and over time the computer learns to tell the
difference between that of a dog and a cat. We call an algorithm like this that separates things into two
piles a binary classifier. There are many different kinds of binary classifiers with a whole host
of advantages and disadvantages but we will start with one that is simple to understand; the
perceptron.
<!--l. 14--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 16--><p class="noindent" ><img 
src="figures/perceptron1.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-19003r8"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;8. </span><span  
class="content">Perceptron Neuron                                                               </span></div><!--tex4ht:label?: x1-19003r8 -->
                                                                                         
                                                                                         
<!--l. 19--><p class="indent" >      </div><hr class="endfigure">
<!--l. 21--><p class="indent" >      A perceptron takes a number of inputs that are binary in nature and produce a single binary output
ie.is this a dog? The figure <a 
href="#x1-19003r8">8<!--tex4ht:ref: perceptron1 --></a> has 3 inputs (<span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub>, <span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub> and <span 
class="zptmcm7m-x-x-120">x</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub>) although, more or fewer inputs may be used.
Each input then is given a weight &#8211; <span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7t-x-x-90">1</span></sub>, <span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7t-x-x-90">2</span></sub> and <span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7t-x-x-90">3</span></sub> in this case &#8211; and the output calculated
thus.
<!--l. 24--><p class="indent" >
<table 
class="align">
                             <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">y </span><span 
class="zptmcm7t-x-x-120">= </span><img 
src="main0x.png" alt="(
|
|||| 0 if &#x2211;i wixi &#x2264; threshhold,
|{
  1 if &#x2211; w x &#x003E; threshhold,
|||       i i i
|||
("  class="left" align="middle"></td>                             <td 
class="align-even"></td>                             <td 
class="align-label"><a 
 id="x1-19004r2"></a>(2)                             </td></tr></table>
<!--l. 31--><p class="indent" >      Used in this fashion, a perceptron can only make simple choices. Raising the threshold makes
the classification tighter while lowering it loosens the classification. Because the output of
a perceptron is binary, for more subtle distinctions, we can use the output of a perceptron
to feed into the input of the next one thus creating a network that is more able to measure
subtlety.
<!--l. 35--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 37--><p class="noindent" ><img 
src="figures/network.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-19005r9"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;9. </span><span  
class="content">Perceptron network                                                              </span></div><!--tex4ht:label?: x1-19005r9 -->
                                                                                         
                                                                                         
<!--l. 40--><p class="indent" >      </div><hr class="endfigure">
<!--l. 42--><p class="indent" >      Varying the weights of the inputs in combination with the threshold for the output allows us to get
different models of classification. The neurons in the first layer are only able to make simple decisions
based on the raw input but because we use their output as the input to the second layer, the
second layer can make more abstract decisions with a degree of subtlety impossible not only
with one perceptron but also with even a single layer of perceptrons. The complexity of the
discrimination by the classifier increasing with both the number and layers of perceptrons in the
network.
<!--l. 46--><p class="indent" >      With the correct weights and threshold values, we can get any binary classifier we want using a set
of perceptrons. That, however, puts us back at our original problem of classifying whether something is a
dog; namely, if we knew what features to look for (i.e.&#x00A0;what weights and threshold to use) it
wouldn&#8217;t be hard explaining to a computer what a dog was. The true innovation comes with using
learning algorithms that don&#8217;t require input from the programmer to set these weights and
thresholds.
<!--l. 50--><p class="indent" >      If we want to use algorithms that can adjust weights and thresholds (otherwise called
biases) automatically, we need some method where a small change in the weight only causes a
small change in the output. Because perceptrons are binary, this is impossible to do with only
perceptrons.
<!--l. 53--><p class="indent" >      A small change in the weight to an input to the perceptron can flip the output entirely. While this
small change in weight can make one of the outputs of the network better, it may also affect the rest of the
network behave in unpredictable ways. Going back to the dog and cat example, while changing the
weight slightly may make it better at recognizing dogs, it may wreak havoc on how cats are
identified.
<!--l. 57--><p class="indent" >      This is where sigmoid neurons come in.
      <h4 class="subsectionHead"><span class="titlemark">3.2    </span> <a 
 id="x1-200003.2"></a>Sigmoid Neuron</h4>
                                                                                         
                                                                                         
<!--l. 3--><p class="noindent" >While perceptrons are effectively step functions, flipping from <span 
class="zptmcm7t-x-x-120">0 </span>to <span 
class="zptmcm7t-x-x-120">1</span>, sigmoids are more smoothed out.
This means that a small change in the weight can lead to a small change in output. The sigmoid function
can be written as
<!--l. 7--><p class="indent" >
<table 
class="align">
                                       <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">&#x03C3; </span><span 
class="zptmcm7t-x-x-120">=</span> <img 
src="main1x.png" alt="---1---
1+ e- z"  class="frac" align="middle"></td>                                       <td 
class="align-even"></td>                                       <td 
class="align-label"><a 
 id="x1-20001r3"></a>(3)                                       </td></tr></table>
<!--l. 11--><p class="indent" >      This means that a sigmoid neuron can be written as
<!--l. 13--><p class="indent" >
<table 
class="align">
                                      <tr><td 
class="align-odd"><img 
src="main2x.png" alt="      1
-------&#x2211;w-x--b
1 + e   i ii"  class="frac" align="middle"></td>                                      <td 
class="align-even"></td>                                      <td 
class="align-label"><a 
 id="x1-20002r4"></a>(4)                                      </td></tr></table>
                                                                                         
                                                                                         
<!--l. 17--><p class="indent" >      where the <span 
class="zptmcm7m-x-x-120">b </span>stands for the bias of every input. While this looks different than the perceptron at first
glance it is just a more smoothed out version of it. One key thing that we lose with the introduction of
sigmoids is the linearity that perceptrons afforded us. What we gain is the ability for our programs to
automatically adjust their weights and biases because a small change in weights does lead to small change
in output as shown in equation <a 
href="#x1-20003r5">5<!--tex4ht:ref: bias --></a>.
<!--l. 22--><p class="indent" >
<table 
class="align">
                                 <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">y </span><span 
class="zptmcm7y-x-x-120">&#x2248;</span><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><img 
src="main3x.png" alt="-&#x2202;y-
&#x2202;wi"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">w</span><sub><span 
class="zptmcm7m-x-x-90">i</span></sub><span 
class="zptmcm7t-x-x-120">+</span><img 
src="main4x.png" alt="&#x2202;y-
&#x2202;b"  class="frac" align="middle"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">b</span></td>                                 <td 
class="align-even"></td>                                 <td 
class="align-label"><a 
 id="x1-20003r5"></a>(5)                                 </td></tr></table>
<!--l. 26--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 28--><p class="noindent" ><img 
src="figures/sigmoid.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-20004r10"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;10. </span><span  
class="content">Sigmoid Function                                                              </span></div><!--tex4ht:label?: x1-20004r10 -->
                                                                                         
                                                                                         
<!--l. 31--><p class="indent" >      </div><hr class="endfigure">
<!--l. 33--><p class="indent" >      More than the exact formula of the sigmoid neuron what matters is the shape. As a result, other
neurons can be used in it&#8217;s stead which retain the property of having a small change in weight lead to a
small change in output. Some of the more popular of these functions (called activation functions) are
RELU and softmax. Each have their own advantages and disadvantages and may even be mixed in the
same neural network
      <h4 class="subsectionHead"><span class="titlemark">3.3    </span> <a 
 id="x1-210003.3"></a>Activation Functions</h4>
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">3.4    </span> <a 
 id="x1-220003.4"></a>Neural Network</h4>
<!--l. 3--><p class="noindent" >A number of these sigmoid neurons (or neurons with other activation functions) can be strung together to
make a neural network. Each neural network has 3 main parts.
<!--l. 6--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 8--><p class="noindent" ><img 
src="figures/neuralNet1.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-22001r11"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;11. </span><span  
class="content">Parts of a Neural Net                                                            </span></div><!--tex4ht:label?: x1-22001r11 -->
                                                                                         
                                                                                         
<!--l. 11--><p class="indent" >      </div><hr class="endfigure">
<!--l. 13--><p class="indent" >      First, we have an input layer. This is all the inputs that go into a neural network and is
usually represented as a vector. Each input adds one to the dimension of the input vector. Even
something like a 2d picture can have its rows stitched together to make one long vector of
inputs.
<!--l. 18--><p class="indent" >      The middle bits are called the hidden layer, not for any profound reason, but just to distinguish
them from the input and output layers. You can have as many hidden middle layers as you want in the
network. The trade off is usually one of efficiency and accuracy. The more hidden layers you have, the
more accurate the output will bebut at the cost of requiring more time to train because there are more
weights to get right. After a point, adding more layers does not improve accuracy in meaningful way
while still taking longer to train. This makes creating a good neural net less of a hard science and more of
an art form.
<!--l. 25--><p class="indent" >      Finally, we have the output layer. This layer usually has one neuron for each thing the classifier can
bin the input into. In the dog and cat case, we would have <span 
class="zptmcm7t-x-x-120">2 </span>output neurons, one that signifies dog and the
other cat. However, the neurons won&#8217;t directly tell us whether the picture contains a dog or a cat but rather
give us two values. One of these values indicates how likely it is for this picture to contain a cat and the
other represents the likelyhood that the picture contains a dog. After that, it is still up to us to decide
on cutoff values to determine whether we will say the picture contains a cat, a dog, both or
neither.
      <h4 class="subsectionHead"><span class="titlemark">3.5    </span> <a 
 id="x1-230003.5"></a>Gradient Descent</h4>
<!--l. 3--><p class="noindent" >So far we&#8217;ve talked about the fact that weights and biases can be adjusted and that it only
works if a small change creates only a small change in output while glossing over how
exactly the computer automatically calculates these weights. Time to peel back that layer!
                                                                                         
                                                                                         
<span class="footnote-mark"><a 
href="main6.html#fn5x0"><sup class="textsuperscript">5</sup></a></span><a 
 id="x1-23001f5"></a> We
use a technique called gradient descent.
<!--l. 8--><p class="indent" >      To start off, we need a set of inputs <span 
class="zptmcm7m-x-x-120">x </span>where we already know the answers <span 
class="zptmcm7m-x-x-120">y</span>. This is called the
training dataset. Once the weights and biases are adjusted we can then use the model to query a set of
inputs that we don&#8217;t know and be reasonably certain that it won&#8217;t give us garbage outputs. To do this
adjustment, we need to define a cost function.
<!--l. 13--><p class="indent" >
<table 
class="align">
                                <tr><td 
class="align-odd"><span 
class="zptmcm7m-x-x-120">C</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">w,b</span><span 
class="zptmcm7t-x-x-120">) =</span> <img 
src="main5x.png" alt="1--
2n"  class="frac" align="middle"><span 
class="zptmcm7v-x-x-120">&#x2211;</span><sub><span 
class="zptmcm7m-x-x-90">x</span></sub><span 
class="zptmcm7y-x-x-120">||</span><span 
class="zptmcm7m-x-x-120">y</span><span 
class="zptmcm7t-x-x-120">(</span><span 
class="zptmcm7m-x-x-120">x</span><span 
class="zptmcm7t-x-x-120">)</span><span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">a</span><span 
class="zptmcm7y-x-x-120">||</span><sup><span 
class="zptmcm7t-x-x-90">2</span></sup></td>                                <td 
class="align-even"></td>                                <td 
class="align-label"><a 
 id="x1-23003r6"></a>(6)                                </td></tr></table>
<!--l. 17--><p class="indent" >      Where <span 
class="zptmcm7m-x-x-120">n </span>is the number of training samples and <span 
class="zptmcm7m-x-x-120">a </span>is the vector of outputs from the network. We
want a set of weights that make the cost as small as possible and we can do that through a method called
gradient descent. The function described here is not the only cost function possible but is a simple one to
start with. To use gradient descent, we can do
<!--l. 22--><p class="indent" >
                                                                                         
                                                                                         
<table 
class="align">
                                       <tr><td 
class="align-odd"><span 
class="zptmcm7t-x-x-120">&#x0394;</span><span 
class="zptmcm7m-x-x-120">v </span><span 
class="zptmcm7t-x-x-120">= </span><span 
class="zptmcm7y-x-x-120">-</span><span 
class="zptmcm7m-x-x-120">&#x03B7;</span><span 
class="zptmcm7y-x-x-120">&#x2207;</span><span 
class="zptmcm7m-x-x-120">C</span></td>                                       <td 
class="align-even"></td>                                       <td 
class="align-label"><a 
 id="x1-23004r7"></a>(7)                                       </td></tr></table>
<!--l. 26--><p class="indent" >      where <span 
class="zptmcm7m-x-x-120">v </span>is the set of weights and biases and <span 
class="zptmcm7m-x-x-120">&#x03B7; </span>is the learning rate. The more aggressive we set <span 
class="zptmcm7m-x-x-120">&#x03B7;</span>
the quicker training will go, but it may end up actually increasing the cost function. So we want an <span 
class="zptmcm7m-x-x-120">&#x03B7; </span>that
is small but not too small.
      <h4 class="subsectionHead"><span class="titlemark">3.6    </span> <a 
 id="x1-240003.6"></a>Convolutional Neural Network</h4>
<!--l. 49--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">3.7    </span> <a 
 id="x1-250003.7"></a>Graph Neural Network</h4>
<!--l. 1--><p class="noindent" >
      <h4 class="subsectionHead"><span class="titlemark">3.8    </span> <a 
 id="x1-260003.8"></a>Model Development</h4>
<!--l. 3--><p class="noindent" >Developing a neural net isn&#8217;t just about figuring out the neurons for the network and adjusting the weights.
The task of making a neural net can be broken up into 3 main parts.
<!--l. 6--><p class="indent" >      <hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
                                                                                         
                                                                                         
<!--l. 8--><p class="noindent" ><img 
src="figures/mlLifecycle.png" alt="PIC"  
width="341" height="341" > <a 
 id="x1-26001r12"></a>
<br />                                                                                         <div class="caption" 
><span class="id">
Figure&#x00A0;12. </span><span  
class="content">Model development                                                             </span></div><!--tex4ht:label?: x1-26001r12 -->
                                                                                         
                                                                                         
<!--l. 11--><p class="indent" >      </div><hr class="endfigure">
<!--l. 13--><p class="indent" >      The first step of any kind of model development is looking at both what kind of data is available as
well as what kind of input we might want to make on the model. The data may be scattered about in many
places and often will require processing before it can be vectorized.
<!--l. 16--><p class="indent" >      In the context of neutrino reconstruction, this may require running monte carlo simulations with
standard software e.g. (LArSoft, NDsim) and then taking the output from those simulations,
processing it into standard images that libraries like pytorch or tensorflow can take as input. It
is also important to think about standardizing the size of those images and thinking about
how to toss out the sheer amount of data that has no hits in it because neutrino events are so
sparse.
<!--l. 19--><p class="indent" >      Once that has been done, we can look at actually implementing a neural network based on that
data. This involves setting out training pipelines which will determine how the data flows, as well as
figuring out the structure of the network that will be made. Tests also have to be written for the network so
that it can be deployed robustly. Once the training with the training dataset is complete, the model has to
be validated with a validation dataset. The validation set will also be a set where the answers are
previously known so we can see how well the model performs on data that it hasn&#8217;t previously been run
on.
<!--l. 25--><p class="indent" >      Once the model has been validated, it can finally be deployed for real world data where we don&#8217;t
have the answers. This is the inference part of the model lifecycle.
      <h4 class="subsectionHead"><span class="titlemark">3.9    </span> <a 
 id="x1-270003.9"></a>Model Optimization</h4>
<!--l. 3--><p class="noindent" >There are two parts where a model can be optimized. The first is the training phase. Models can take a
long time to train even if a lot of data is available which means it is often worth it to optimize the training
phase. This sort of optimization is called hyperparameter optimization because the actual hyperparameters
(weights and biases) aren&#8217;t being tweaked but rather the parameters that guide how they are formed. It
involves manipulating the structure of the network as well as changing factors such as the learning rate.
                                                                                         
                                                                                         
The difference between a naive implementation and an optimized one may lead to a speedup of hours for
the training.
<!--l. 10--><p class="indent" >      Training isn&#8217;t however what a network is spending most of its time doing. Most of the time a
network is used to query for answers, i.e.&#x00A0;inference. Inference speedups can be done through
a number of ways such as using more specialized hardware like FPGA&#8217;s or working with
TensorRT optimization. That can bring down the time it takes to query the model for information
which can vastly affect number of events being processed in any time period thus increasing
throughput.
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
                                                                                         
      <h3 class="likesectionHead"><a 
 id="x1-28000"></a>References</h3>
<a 
 id="Q1-1-43"></a>
<!--l. 1--><p class="noindent" >
    <div class="thebibliography">
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XTimaeus"></a>[1] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Plato John&#x00A0;Warrington.  <span 
class="ptmri7t-x-x-120">Tmaeus - Plato ; edited and translated with an introduction by</span>
    <span 
class="ptmri7t-x-x-120">John Warrington</span>. Dent ; Dutton, 1965.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDalton"></a>[2] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Harold  Hartley.     John  dalton,  f.r.s.  (1766-1844)  and  the  atomic  theory-a  lecture  to
    commemorate  his  bicentenary.     <span 
class="ptmri7t-x-x-120">Proceedings  of  the  Royal  Society  of  London.  Series  B,</span>
    <span 
class="ptmri7t-x-x-120">Biological Sciences</span>, 168(1013):335&#8211;359, 1967.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XelectronDiscovery"></a>[3] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>J.&#x00A0;J. Thomson. The electron. <span 
class="ptmri7t-x-x-120">The Scientific Monthly</span>, 20(2):113&#8211;115, 1925.
</p>
    </div>
       
</body></html> 

                                                                                         


